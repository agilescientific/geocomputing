{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Pandas\n",
    "\n",
    "Pandas is a very useful library for working with tabular data. If it is something that would fit into a spreadsheet or csv file, then this is a great way to deal with it. The library is big, with [extensive documentation](https://pandas.pydata.org/pandas-docs/stable/), so this is going to just scratch the surface, and hopefully help you towards being able to use it to automate some of your routine data processing tasks that normally involve Excel.\n",
    "\n",
    "We will first import the libraries that we need. As is common in scientific python, we will need `numpy` and `matplotlib`'s `pyplot`. In addition, we will import `pandas` and `seaborn` (the latter is another plotting library built on `matplotlib`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "The easiest way to load data using `pandas` is with the relevant `read_*` method. There are a range of these to read a variety of data formats and files, including `read_csv`, `read_sql`, `read_clipboard`. For our dataset, we will use `read_excel`. They all work in roughly the same manner, but may have a wide range of additional arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('./data/RPC_4_lithologies.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a DataFrame. Each DataFrame has a number of named Series, which are analogous to columns in a spreadsheet, and Indexes, which are analogous to rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `RPC` column is a unique identifier, which makes it potentially suitable for use as an index, but it is unfortunately not sequential, so we will keep the default one instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting data\n",
    "\n",
    "Pandas offers a few useful ways to see what data we have available in a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(8) # An int here will display that many rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some statistical information for numerical fields can be found using `describe`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in many cases we get back a new DataFrame from a given function. This can be treated the same as any other DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting data\n",
    "\n",
    "### Selecting by column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Vs # equivalent to df['Vs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Vp', 'Vs', 'Lithology']] # pass a list of column names to select a subset of columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting by row\n",
    "\n",
    "When selecting by row, either the index or the position can be used.\n",
    "\n",
    "This is selecting by index. Note that the stop value is included, unlike standard python slicing. It is also possible to use a timeseries as the index, which will slice differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.loc[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting by position has the same behaviour as standard python slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of this dataset, they are very similar, but `loc` can be be used to access things such as times or dates, rather than integer positions. `iloc` is the integer location of the DataFrame, which is just the position.\n",
    "\n",
    "Just like in numpy, boolean conditions can be used to select subsets of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Lithology'] == 'sandstone'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sandstones = df.loc[df['Lithology'] == 'sandstone']\n",
    "sandstones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Plotting\n",
    "\n",
    "The DataFrame has a built-in `plot` function, which can plot given Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[['Vp', 'Vs']].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `kind` keyword can change the type of plot that is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sandstones['Vp'].plot(kind='hist', bins=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If desired, a given Series (or set of Series) can be plotted using standard `matplotlib.pyplot` functions as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(df['Vp'], label='Vp')\n",
    "ax.plot(df['Vs'], label='Vs')\n",
    "ax.set_ylabel('Velocity [m/s]')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "_ = ax.hist(sandstones['Vp'], bins=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "low_densities = df.loc[df['Rho'] <= 2000]\n",
    "low_densities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df.Lithology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(low_densities['Vp'], low_densities['Rho'])\n",
    "ax.set_xlabel('Velocity [m/s]')\n",
    "ax.set_ylabel('Density [g/cm3]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing null values\n",
    "\n",
    "Notice that in the output from `df.describe`, the `Rho` and `Rho_n` columns have a lower count (752) than the remaining columns (800). This implies that there is missing data in those columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can remove the rows containing missing data easily with `.dropna`. By default it drops rows (indices) with a NaN, but it can do it for columns too.\n",
    "\n",
    "`inplace=True` gets us the same effect as `df = df.dropna()`. This option exists for a number of DataFrame methods. _Use it with caution_: it changes the original DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have dropped values, we will now get gaps in our index, at around 500 to 600."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can make using slices not work as expected, so we will reset the index to remove the gap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation\n",
    "\n",
    "A very useful tool in pandas is grouping by specific values in a field. This uses the groupby, followed by the function that you wish to know about the group. Common options are `mean`, `median`, `sum`, `count`, `max`, and `min`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Lithology').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Lithology').median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`groupby` is a very flexible, powerful tool. The [documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html) is extensive, and this demo will not go into it in detail. In this case, grouping by 'Lithology' seems natural, because we might expect the different lithologies to have different P- and S-wave velocities, along with a different Rho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby(['Lithology'])[['Vs', 'Vp', 'Rho']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now obtain some aggregate stats per group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.agg([np.size, np.mean, np.median, np.std]).T # The .T pivots the table so it prints more compactly here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also group by multiple columns. In this case we will get a count of `Vp` values when we group by `Lithology` and then `Description`.\n",
    "\n",
    "In order to see how this works completely, we will temporarily overwrite the number of rows shown using a context manager. Notice that each record is grouped into a lithology and then a description, and we get the count of each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None):\n",
    "    print(df.groupby(['Lithology', 'Description'])['Vp'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, we can see that the limestone consistently has only one or two `Vp` values for each description, while the shales are more variable. The sandstones have fewer different descriptions, but some of those have many `Vp` values associated with them. The dolomites have fewer descriptions again, but all have at least 13 `Vp` values. This may affect what sort of statistics we can derive from this dataset.\n",
    "\n",
    "We can also do something like plot the median of our `Vp`, `Vs` and `rho` for each lithology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "grouped.agg(np.median).T.plot(marker='o', lw=0, ax=ax)\n",
    "#grouped.agg(np.min).T.plot(marker='*', lw=0, ax=ax)\n",
    "#grouped.agg(np.max).T.plot(marker='+', lw=0, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dolomites have the highest median values for both `Vp` and `Vs`, with limestone notably lower. Shale and sandstone are between these, and are quite similar in value. The `Rho` has less scatter.\n",
    "\n",
    "## Adding data\n",
    "\n",
    "Recall in the _Intro to Functions_ notebook we created a function to calculate acoustic impedance, given a rho and Vp. We can use this to create a new `impedance` Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impedance(rho, vp):\n",
    "    \"\"\"\n",
    "    Calculate acoustic impedance from Rho and Vp.\n",
    "\n",
    "    args:\n",
    "        rho: [float] density\n",
    "        vp: [float] p-wave velocity\n",
    "\n",
    "    returns:\n",
    "        z: [float] acoustic impedance\n",
    "    \"\"\"\n",
    "    z = rho * vp\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to add it to the DataFrame, we use a similar approach as with dictionaries, where we assign a specific column the values. If the column does not exist, it will be created for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impedance(df['Rho'], df['Vp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Impedance'] = impedance(df['Rho'], df['Vp'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also see how different the result of the Rho calculated by Gardner's equation ( $ \\rho = 310\\ V_\\mathrm{P}^{\\,0.25}\\ \\ \\mathrm{kg}/\\mathrm{m}^3 $ ) is from the measured Rho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gardner(vp, alpha=310, beta=0.25):\n",
    "    '''\n",
    "    Calculate Gardner's equation, given a Vp. Alpha and beta are optional.\n",
    "    \n",
    "    Args:\n",
    "        vp: [float] p-wave velocity\n",
    "        alpha: [float]\n",
    "        beta: [float]\n",
    "        \n",
    "    Returns:\n",
    "        rho: [float] density\n",
    "    '''\n",
    "    rho = alpha * vp**beta\n",
    "    return rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Rho_gardner'] = gardner(df['Vp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot how far off the Gardner equation gets us by looking at the difference from measured values, and then saving that error to the df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df.Rho - df.Rho_gardner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Gardner_error'] = df.Rho - df.Rho_gardner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Rho` and `Rho_n` are very similar, so we will remove `Rho_n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df.Rho - df.Rho_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(df['Rho'], bins=50, alpha=0.7, label='rho')\n",
    "ax.hist(df['Rho_gardner'], bins=50, alpha=0.7, label='Gardner rho')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Rho_n'], axis=1, inplace=True) # axis=1 means that we want to drop columns.\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying functions per row\n",
    "\n",
    "Sometimes we may have a function that requires input per row. An example might be where the lithology affects the calculation that we want to use by means of an optional argument.\n",
    "\n",
    "We will change the parameters of Gardner's equation by the lithology of the sample. This requires a function that will work on the row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_gardner(row):\n",
    "    if row['Lithology'] == 'dolomite':\n",
    "        alpha, beta = 250, 0.28\n",
    "    elif row['Lithology'] == 'limestone':\n",
    "        alpha, beta = 250, 0.28\n",
    "    elif row['Lithology'] == 'shale':\n",
    "        alpha, beta = 350, 0.25\n",
    "    elif row['Lithology'] == 'sandstone':\n",
    "        alpha, beta = 380, 0.23\n",
    "    else:\n",
    "        alpha, beta = 310, 0.25\n",
    "    return gardner(row['Vp'], alpha, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this function, we can work through the DataFrame row-wise, and `apply` the function on each row. The resulting Series can be added to `df` in the normal way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Rho_v_gardner'] = df.apply(variable_gardner, axis=1)\n",
    "df['VGardner_error'] = df.Rho - df.Rho_v_gardner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(df['Rho'], bins=50, alpha=0.7, label='rho')\n",
    "ax.hist(df['Rho_gardner'], bins=50, alpha=0.7, label='Gardner rho')\n",
    "ax.hist(df['Rho_v_gardner'], bins=50, alpha=0.7, label='Variable Gardner rho')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With enough knowledge of the different sensible ranges for `alpha` and `beta` for a given lithology, we can improve the fit of the `variable_gardner` results for each lithology. Currently we are overestimating our rho fairly noticeably.\n",
    "\n",
    "# Plotting with Seaborn\n",
    "\n",
    "Seaborn is a nice wrapper around Matplotlib with a focus on statistical plots. It makes some things much simpler than in standard Matplotlib. We can start by selecting some data that we are interested in from the available columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot = ['Rho', 'Rho_gardner', 'Rho_v_gardner', 'Vp', 'VGardner_error']\n",
    "g = sns.PairGrid(df, hue='Lithology', vars=to_plot, diag_sharey=False)\n",
    "g.map_lower(sns.scatterplot, alpha=0.4)\n",
    "g.map_upper(sns.kdeplot, alpha=0.4)\n",
    "g.map_diag(sns.kdeplot)\n",
    "g.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should now give us a better handle on the reasonable ranges in which we can expect our densities and velocities to vary based on lithology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing files\n",
    "\n",
    "Since we processed our data (by adding new calculated values), we should write these changes out to a file. Luckily this is very straightforward, using one of the `.to_*` methods. Common ones to store data for future use are `.to_csv`, `.to_excel`, `.to_hdf`. It is also possible to interact with SQL databases or convert to other in-memory formats such as a dict or xarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_xarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When writing to Excel or csv, the index will be added as a column. Should you not need that (for this example they are simply ascending numbers), then use `index=False` in the call to your `to_*` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('./data/edited_RPC_4_lithologies.xlsx', sheet_name='lithologies', index=False)\n",
    "df.to_csv('./data/edited_RPC_4_lithologies.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use a context manager to append to an existing Excel file, or to write to multiple sheets within it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter('./data/edited_RPC_4_lithologies.xlsx', mode='a') as writer:\n",
    "    df.to_excel(writer, sheet_name='processed RPC4', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "<img src=\"https://avatars1.githubusercontent.com/u/1692321?v=3&s=200\" style=\"float:center\" width=\"40px\" />\n",
    "<p><center>© 2021 <a href=\"http://www.agilegeoscience.com/\">Agile Geoscience</a> — <a href=\"https://creativecommons.org/licenses/by/4.0/\">CC-BY</a></center></p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geocomp",
   "language": "python",
   "name": "geocomp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
