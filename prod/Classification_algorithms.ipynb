{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ec33039",
   "metadata": {},
   "source": [
    "# Classification algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5846ab",
   "metadata": {},
   "source": [
    "The objectives of this notebook are:\n",
    "\n",
    "* Learn and compare the basic model families in shallow machine learning classication problems.Specifically we'll look at: \n",
    "\n",
    "    * Logistic regression,\n",
    "    \n",
    "    * Nearest neighbours models, \n",
    "    \n",
    "    * Support vector machines,\n",
    "    \n",
    "    * Decision Trees, and Ensemble models.\n",
    "    \n",
    "    * Neural Networks (very briefly)\n",
    "\n",
    "* Understand the key hyperparameters that control how these models learn.\n",
    "\n",
    "* Use decision boundaries to visualize how models make predictions.\n",
    "\n",
    "* Discuss the properties of an appropriate predictive model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5351df08",
   "metadata": {},
   "source": [
    "## The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d0dc20",
   "metadata": {},
   "source": [
    "First we'll import some data. I'm using an extract from the Rock Property Catalog, https://subsurfwiki.org/wiki/Rock_Property_Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27906f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('https://geocomp.s3.amazonaws.com/data/RPC_4_lithologies_original.csv')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9b28e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9389a3",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Remove the units from the column names so we can refer to them just using their abbreviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be4234a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514445c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_names = {'Vp [m/s]': 'Vp', 'Vs [m/s]': 'Vs', 'Rho [g/cmÂ³]': 'Rho'}\n",
    "df = df.rename(new_names, axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f201b00b",
   "metadata": {},
   "source": [
    "We'll start our classification journey by using the logistic regression algorithm one a variable and two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a7e713",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Logistic regression is similar to linear regression, but instead of predicting a continuous variable, it predicts whether something is true or false. It is a classification algorithm. \n",
    "\n",
    "Instead of fitting a line to the data, Logistic regression fits a logistic function (a.k.a sigmoid) to the data. The model then is a probability function used to classify new data.\n",
    "\n",
    "$$f(x) = \\frac{1}{1+e^{-\\textbf{x}}}$$\n",
    "\n",
    "$$f(x) = \\frac{1}{1+e^{-(\\textbf{wx}+b)}}$$\n",
    "\n",
    "It has many uses in data analysis and machine learning, especially in data transformations. The curve goes from zero to one. It tells you the probability that a sample is a class of interest or not. Instead of using a least-squares type loss function, it uses a maximum likelihood function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fdb75a",
   "metadata": {},
   "source": [
    "# EXERCISE: \n",
    "\n",
    "- Write a function called `logistic` that takes x, w, and b as arguments and returns the value of the logistic.\n",
    "\n",
    "- Make a plot of the logistic function from x = -10 to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8204e995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(x, w=1, b=0):\n",
    "    \"\"\"Logistic function.\n",
    "    Args:\n",
    "        x (array or int): input\n",
    "        w (float or array): the weights of the logistic\n",
    "        b (float): the intercept (or bias)\n",
    "    \"\"\"\n",
    "    # goes here your_equation \n",
    "    return # your equation goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48bb634",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = # your code goes here. \n",
    "plt.plot(x, sigmoid(x), 'o-')\n",
    "plt.title('The logistic function where w=1, b=0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224ba0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(x, w=1, b=0):\n",
    "    \"\"\"Logistic function.\n",
    "    Args:\n",
    "        x (array or int): input\n",
    "        w (float or array): the weights of the logistic\n",
    "        b (float): the intercept (or bias)\n",
    "    \"\"\"\n",
    "    term = np.exp(-(w * x + b))\n",
    "    return 1 / (1 + term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0ab090",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-10, 10)\n",
    "plt.plot(x, sigmoid(x), 'o-')\n",
    "plt.title('The logistic function where w=1, b=0')\n",
    "\n",
    "# Also note we can use the expit function in scipy for this (but we don't have to!)\n",
    "from scipy.special import expit\n",
    "y = expit(x)\n",
    "plt.plot(x, y)\n",
    "plt.plot(x, sigmoid(x), 'o-')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca135ac",
   "metadata": {},
   "source": [
    "## Select the data for this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c02986",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Vp']  # a single feature\n",
    "classes = ['shale', 'dolomite']  # two classes\n",
    "df_LR = df.loc[df['Lithology'].isin(classes)]\n",
    "\n",
    "X = df_LR[features].values\n",
    "y = df_LR['Lithology'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43258744",
   "metadata": {},
   "source": [
    "## How are these two classes distributed along the `Vp` dimension?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73ba513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a custom color palette for seaborn\n",
    "import seaborn as sns\n",
    "colors = ['goldenrod', 'darkseagreen', 'cornflowerblue', 'blueviolet']\n",
    "palette = sns.color_palette(colors)\n",
    "hue_order = df['Lithology'].unique()\n",
    "_ = sns.histplot(data=df_LR, x='Vp', kde=True, hue='Lithology', palette=palette, hue_order=hue_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5c15e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn \n",
    "\n",
    "print(sklearn.__version__)\n",
    "\n",
    "# Make sure we can see all of the model details.\n",
    "sklearn.set_config(print_changed_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aace9097",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_lr, X_val_lr, y_train_lr, y_val_lr = train_test_split(X, y, test_size=0.2, random_state=32)\n",
    "\n",
    "X_train_lr.shape, X_val_lr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8084450",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "model = LogisticRegressionCV()\n",
    "\n",
    "model.fit(X_train_lr, y_train_lr)\n",
    "\n",
    "y_pred_lr = model.predict(X_val_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684d691b",
   "metadata": {},
   "source": [
    "We can now use `model.predict()` to perform our classifications, but if we choose we can also take it's learned coefficients to studied the logistic curve that we fitted to the data. The sigmoid alone, however does not provide the classification directly, but it's illustrative to inspect it relative to the data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20dc711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our own logistic function from the learned model parameters\n",
    "y_test_lr = sigmoid(X_val_lr * model.coef_ + model.intercept_).ravel()\n",
    "plt.scatter(X_val_lr, y_test_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c95e2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "from ml_utils import logistic_progression\n",
    "\n",
    "@interact(cutoff=np.arange(0, 1.0, 0.05))\n",
    "def logistic_regression_plot(cutoff=0.5):\n",
    "    logistic_progression(model, X_val_lr, y_val_lr, y_test_lr, cutoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5d2eee",
   "metadata": {},
   "source": [
    "# Add more features and more classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225e0b12",
   "metadata": {},
   "source": [
    "## Make a new X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3460f6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Vp', 'Rho']  # two features\n",
    "X = df[features].values  # four classes\n",
    "y = df['Lithology']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5071be27",
   "metadata": {},
   "source": [
    "## Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbac3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e075f044",
   "metadata": {},
   "source": [
    "## Standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922e3db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f16a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a scatter plot\n",
    "scatter = sns.relplot(data=df, x='Vp', y='Rho', hue='Lithology', s=80, alpha=0.5, height=6, \n",
    "                      palette=palette, hue_order=hue_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52f6c55",
   "metadata": {},
   "source": [
    "## Logistic Regression for 4 classes and 2 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ba6bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "model = LogisticRegressionCV()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b32c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clf_utils import val_vs_pred_scatter\n",
    "\n",
    "val_vs_pred_scatter(X_val, y_val, y_pred, palette, hue_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc1a9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f053043b",
   "metadata": {},
   "source": [
    "## Decision regions\n",
    "\n",
    "We can consider the cutoffs in higher dimensions as a sort of decision regions. We'll use this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36456c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clf_utils import show_decision_regions\n",
    "\n",
    "show_decision_regions(model, X_train, y_train, X_val, y_val, palette, hue_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e251d1",
   "metadata": {},
   "source": [
    "## k-NN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19147e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=5)  # the default it 5\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_val)\n",
    "\n",
    "accuracy_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bd4dff",
   "metadata": {},
   "source": [
    "## What about some different values of k?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e11df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "\n",
    "@interact(n=[1, 3, 5, 10, 20, 30, 50, 100, 150])\n",
    "def decision_boundaries(n):\n",
    "    clf = KNeighborsClassifier(n_neighbors=n)\n",
    "    show_decision_regions(clf, X_train, y_train, X_val, y_val, palette, hue_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89d63c4",
   "metadata": {},
   "source": [
    "# EXERCISE:\n",
    "\n",
    "    - Do high or low values if `n` create a smoothing effect?\n",
    "    - What value of `n_neighbors` gives the highest accuracy?\n",
    "    - How many times is shale being \"confused\" as sandstone?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d35514",
   "metadata": {},
   "source": [
    "# Support-vector machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff8ed07",
   "metadata": {},
   "source": [
    "## Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470a42f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC(kernel='linear')\n",
    "\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svc.predict(X_val)\n",
    "\n",
    "accuracy_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa1c007",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(C=[0.01, 0.1, 1.0, 10, 100])\n",
    "def decision_boundaries(C=1):\n",
    "    clf = SVC(kernel='linear', C=C)\n",
    "    show_decision_regions(clf, X_train, y_train, X_val, y_val, palette, hue_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679b5775",
   "metadata": {},
   "source": [
    "## Non-linear SVM \n",
    "\n",
    "If we employ the **kernel trick** we can fit a nonlinear model. Scikit-learn's `SVC` actually uses this by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60b930b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC(C=1)  # Default is kernel='rbf'\n",
    "\n",
    "svc.fit(X_val, y_val)\n",
    "\n",
    "y_pred = svc.predict(X_val)\n",
    "\n",
    "accuracy_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50ae671",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(C=[0.001, 0.001, 0.01, 0.1, 1.0, 10, 100, 1000, 1e4])\n",
    "def decision_boundaries(C=1):\n",
    "    clf = SVC(kernel='rbf', C=C)  # default\n",
    "    show_decision_regions(clf, X_train, y_train, X_val, y_val, palette, hue_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b1d153",
   "metadata": {},
   "source": [
    "# EXERCISE:\n",
    "\n",
    "    - What values of C give the highest accuracy?\n",
    "    - What value of C *looks* to be yield the most appropriate model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1355b5b",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64dcd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=3)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_val)\n",
    "\n",
    "accuracy_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5d89bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clf_utils import lithology_tree\n",
    "\n",
    "lithology_tree(clf, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5592a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(max_depth=[2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "def decision_boundaries(max_depth=4):\n",
    "    clf = DecisionTreeClassifier(max_depth=max_depth)\n",
    "    show_decision_regions(clf, X_train, y_train, X_val, y_val, palette, hue_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba882a95",
   "metadata": {},
   "source": [
    "## Random Forests and Ensemble methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740bcef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=3).fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_val)\n",
    "\n",
    "accuracy_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ba4422",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(max_depth=np.arange(2, 10), min_samps_leaf=np.arange(1,6))\n",
    "def decision_boundaries(max_depth=3, min_samps_leaf=3):\n",
    "    clf = RandomForestClassifier(max_depth=max_depth, min_samples_leaf=min_samps_leaf)\n",
    "    show_decision_regions(clf, X_train, y_train, X_val, y_val, palette, hue_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb8c609",
   "metadata": {},
   "source": [
    "## Boosted trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a5fadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, ExtraTreesClassifier\n",
    "\n",
    "clf = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_val)\n",
    "\n",
    "accuracy_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7890b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(max_depth=np.arange(2, 10), min_samps_leaf=np.arange(1,6))\n",
    "def decision_boundaries(max_depth=3, min_samps_leaf=3):\n",
    "    clf = GradientBoostingClassifier(max_depth=max_depth, min_samples_leaf=min_samps_leaf)\n",
    "    show_decision_regions(clf, X_train, y_train, X_val, y_val, palette, hue_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a64d33",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41149eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=[10, 10],\n",
    "                    learning_rate='constant',\n",
    "                    alpha=0.001,\n",
    "                    max_iter=5000,\n",
    "                    solver='adam',\n",
    "                    random_state=42,\n",
    "                   )\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_val)\n",
    "\n",
    "accuracy_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36e846b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(hl1=np.arange(2, 15, 1))\n",
    "def decision_boundaries(hl1=3):\n",
    "    clf = MLPClassifier(hidden_layer_sizes=[hl1],\n",
    "                    learning_rate='constant',\n",
    "                    alpha=0.001,\n",
    "                    max_iter=5000,\n",
    "                    solver='adam',\n",
    "                    random_state=42,\n",
    "                   )\n",
    "    show_decision_regions(clf, X_train, y_train, X_val, y_val, palette, hue_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8920306",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Choosing the right estimator\n",
    "\n",
    "Often the hardest part of solving a machine learning problem can be finding the right estimator for the job.\n",
    "\n",
    "This is a good place to start ([here](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html) is a clickable version):\n",
    "\n",
    "<img src=\"https://scikit-learn.org/stable/_static/ml_map.png\"></img>\n",
    "\n",
    "---\n",
    "\n",
    "Different estimators are better suited for different types of data and different problems. For a classifier comparison (below) check the source code [here](http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html)\n",
    "\n",
    "<img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_classifier_comparison_001.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c95a7a",
   "metadata": {},
   "source": [
    "###  [Check out this paper with a comparison of many classifiers](https://arxiv.org/abs/1708.05070)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c402e2f",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "- Choosing a model mean you are making an interpretation\n",
    "- You need to know the key hyperparameters that effect how models learn\n",
    "- KNN and SVM models have few hyperparameters\n",
    "- Decision Trees and Neural Networks have more hyperparameters so they can be harder to \"tune\"\n",
    "- All models can be underfit and overfit to your data\n",
    "\n",
    "# Next steps\n",
    "\n",
    "- Classification Reports and Confusion Matricies. Accuracy is usually not enough. We need more detailed performance metrics.\n",
    "- Chosen hyperparameters visually is not good enough, we need to get systematic."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geoml",
   "language": "python",
   "name": "geoml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
