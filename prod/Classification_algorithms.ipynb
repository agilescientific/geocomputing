{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ec33039",
   "metadata": {},
   "source": [
    "# Classification algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5846ab",
   "metadata": {},
   "source": [
    "The objectives of this notebook are to learn and compare the basic model families in shallow machine learning classication problems.\n",
    "\n",
    "We'll look at the following types of models: \n",
    "\n",
    "* Logistic regression,\n",
    "\n",
    "* Nearest neighbours models, \n",
    "    \n",
    "* Support vector machines,\n",
    "    \n",
    "* Decision Trees, and Ensemble models.\n",
    "    \n",
    "* Neural Networks (very briefly)\n",
    "\n",
    "For each model, we will:\n",
    "\n",
    "* Describe the key hyperparameters that control how these models learn.\n",
    "\n",
    "* Visualize decision boundaries study how models make predictions.\n",
    "\n",
    "* Discuss the properties of an appropriate predictive model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5351df08",
   "metadata": {},
   "source": [
    "## The dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d0dc20",
   "metadata": {},
   "source": [
    "First we'll import some data. I'm using an extract from the Rock Property Catalog, https://subsurfwiki.org/wiki/Rock_Property_Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b27906f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RPC</th>\n",
       "      <th>Vp [m/s]</th>\n",
       "      <th>Vs [m/s]</th>\n",
       "      <th>Rho [g/cm³]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>800.000000</td>\n",
       "      <td>800.000000</td>\n",
       "      <td>800.000000</td>\n",
       "      <td>752.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>102276.178750</td>\n",
       "      <td>3798.453532</td>\n",
       "      <td>2113.480238</td>\n",
       "      <td>2.316455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1537.850865</td>\n",
       "      <td>1114.387572</td>\n",
       "      <td>750.387638</td>\n",
       "      <td>0.255653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>100031.000000</td>\n",
       "      <td>1490.711927</td>\n",
       "      <td>441.270000</td>\n",
       "      <td>1.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>101112.750000</td>\n",
       "      <td>3010.248889</td>\n",
       "      <td>1520.975000</td>\n",
       "      <td>2.117000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>102081.500000</td>\n",
       "      <td>3675.150000</td>\n",
       "      <td>2198.100000</td>\n",
       "      <td>2.390000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>103284.750000</td>\n",
       "      <td>4664.592178</td>\n",
       "      <td>2733.805000</td>\n",
       "      <td>2.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>105090.000000</td>\n",
       "      <td>5993.000000</td>\n",
       "      <td>3665.000000</td>\n",
       "      <td>2.780000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 RPC     Vp [m/s]     Vs [m/s]  Rho [g/cm³]\n",
       "count     800.000000   800.000000   800.000000   752.000000\n",
       "mean   102276.178750  3798.453532  2113.480238     2.316455\n",
       "std      1537.850865  1114.387572   750.387638     0.255653\n",
       "min    100031.000000  1490.711927   441.270000     1.750000\n",
       "25%    101112.750000  3010.248889  1520.975000     2.117000\n",
       "50%    102081.500000  3675.150000  2198.100000     2.390000\n",
       "75%    103284.750000  4664.592178  2733.805000     2.530000\n",
       "max    105090.000000  5993.000000  3665.000000     2.780000"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('https://geocomp.s3.amazonaws.com/data/RPC_4_lithologies_original.csv')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69b4acb",
   "metadata": {},
   "source": [
    "We are going to drop the rows with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da9b28e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9389a3",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "### EXERCISE\n",
    "\n",
    "Remove the units e.g. `[m/s]`, from any column names with units so we only need to refer to a column using its abbreviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be4234a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "514445c8",
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "new_names = {'Vp [m/s]': 'Vp', 'Vs [m/s]': 'Vs', 'Rho [g/cm³]': 'Rho'}\n",
    "df = df.rename(new_names, axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f201b00b",
   "metadata": {},
   "source": [
    "We'll start our discussion of classification by using the logistic regression algorithm one a variable and two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a7e713",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "Logistic regression is similar to linear regression, but instead of predicting a continuous variable, it predicts whether something is true or false. It is a classification algorithm. \n",
    "\n",
    "Instead of fitting a line to the data, Logistic regression fits a logistic function (a.k.a sigmoid) to the data. The model then is a probability function used to classify new data.\n",
    "\n",
    "$$f(x) = \\frac{1}{1+e^{-(\\textbf{wx}+b)}}$$\n",
    "\n",
    "It has many uses in data analysis and machine learning, especially in data transformations. The curve goes from zero to one. It tells you the probability that a sample is a class of interest or not. Instead of using a least-squares type loss function, it uses a maximum likelihood function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fdb75a",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "### EXERCISE\n",
    "\n",
    "- Write a function called `logistic` that takes x, w, and b as arguments and returns the value of the logistic.\n",
    "\n",
    "- Make a plot of the logistic function from x = -10 to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d06f3a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def logistic(x, w=1, b=0):\n",
    "    \n",
    "    # Your code goes here\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f48bb634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "224ba0fb",
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(x, w=1, b=0):\n",
    "    \"\"\"Logistic function.\n",
    "    Args:\n",
    "        x (array or int): input\n",
    "        w (float or array): the weights of the logistic\n",
    "        b (float): the intercept (or bias)\n",
    "    \"\"\"\n",
    "    term = np.exp(-(w * x + b))\n",
    "    return 1 / (1 + term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0ab090",
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "x = np.arange(-10, 10)\n",
    "plt.plot(x, sigmoid(x), 'o-')\n",
    "plt.title('The logistic function where w=1, b=0')\n",
    "\n",
    "# Note, we can use the expit function in scipy for this\n",
    "from scipy.special import expit\n",
    "y = expit(x)\n",
    "plt.plot(x, y)\n",
    "plt.plot(x, sigmoid(x), 'o-')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca135ac",
   "metadata": {},
   "source": [
    "## Make X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c02986",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Vp']  # A 1-D feature space.\n",
    "classes = ['shale', 'dolomite']  # Two classes.\n",
    "df_LR = df.loc[df['Lithology'].isin(classes)]\n",
    "\n",
    "X = df_LR[features].values\n",
    "y = df_LR['Lithology'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73ba513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Create a custom color palette for seaborn.\n",
    "colors = ['goldenrod', 'darkseagreen', 'cornflowerblue', 'blueviolet']\n",
    "palette = sns.color_palette(colors)\n",
    "hue_order = df['Lithology'].unique()\n",
    "\n",
    "_ = sns.histplot(data=df_LR, x='Vp', kde=True, hue='Lithology', palette=palette, hue_order=hue_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5c15e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn \n",
    "\n",
    "print(sklearn.__version__)\n",
    "\n",
    "# Make sure we can see all of the model details.\n",
    "sklearn.set_config(print_changed_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aace9097",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_lr, X_val_lr, y_train_lr, y_val_lr = train_test_split(X, y, test_size=0.2, random_state=32)\n",
    "\n",
    "X_train_lr.shape, X_val_lr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67d2818",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8084450",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "model = LogisticRegressionCV()\n",
    "\n",
    "model.fit(X_train_lr, y_train_lr)\n",
    "\n",
    "y_pred_lr = model.predict(X_val_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684d691b",
   "metadata": {},
   "source": [
    "We can now use `model.predict()` to perform our classifications, but if we choose we can also take it's learned coefficients to studied the logistic curve that we fitted to the data. The sigmoid alone, however does not provide the classification directly, but it's illustrative to inspect it relative to the data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20dc711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our own logistic function from the learned model parameters.\n",
    "y_test_lr = sigmoid(X_val_lr * model.coef_ + model.intercept_).ravel()\n",
    "plt.scatter(X_val_lr, y_test_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c95e2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "from mlutils import logistic_progression\n",
    "\n",
    "@interact(cutoff=np.arange(0, 1.0, 0.05))\n",
    "def logistic_regression_plot(cutoff=0.5):\n",
    "    logistic_progression(model, X_val_lr, y_val_lr, y_test_lr, cutoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5d2eee",
   "metadata": {},
   "source": [
    "## More features and more classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225e0b12",
   "metadata": {},
   "source": [
    "## Make X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3460f6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Vp', 'Rho']  # A 2-D feature space. \n",
    "X = df[features].values  # Including all four classes.\n",
    "y = df['Lithology']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5071be27",
   "metadata": {},
   "source": [
    "## Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbac3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e075f044",
   "metadata": {},
   "source": [
    "## Standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922e3db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f16a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a scatter plot.\n",
    "scatter = sns.relplot(data=df, x='Vp', y='Rho', hue='Lithology', s=80, alpha=0.5, height=6, \n",
    "                      palette=palette, hue_order=hue_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52f6c55",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ba6bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "model = LogisticRegressionCV()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b32c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlutils import val_vs_pred_scatter\n",
    "\n",
    "val_vs_pred_scatter(X_val, y_val, y_pred, palette, hue_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc1a9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f053043b",
   "metadata": {},
   "source": [
    "## Plotting the decision regions\n",
    "\n",
    "The cutoff we say in one-dimension is expressed as decision boundaries in 2D. We can visualize these decision regions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36456c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlutils import show_decision_regions\n",
    "\n",
    "show_decision_regions(model, X_train, y_train, X_val, y_val, palette, hue_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e251d1",
   "metadata": {},
   "source": [
    "## k-NN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19147e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=5)  # the default it 5\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_val)\n",
    "\n",
    "accuracy_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bd4dff",
   "metadata": {},
   "source": [
    "What about some different values of k?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e11df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "\n",
    "@interact(n=[1, 3, 5, 10, 20, 30, 50, 100, 150])\n",
    "def decision_boundaries(n):\n",
    "    clf = KNeighborsClassifier(n_neighbors=n)\n",
    "    show_decision_regions(clf, X_train, y_train, X_val, y_val, palette, hue_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89d63c4",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "### EXERCISE:\n",
    "\n",
    "- Do high or low values if `n` create a smoothing effect?\n",
    "- What value of `n_neighbors` gives the highest accuracy?\n",
    "- How many times is shale being \"confused\" as sandstone?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d35514",
   "metadata": {},
   "source": [
    "## Support-vector machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff8ed07",
   "metadata": {},
   "source": [
    "## Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470a42f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC(kernel='linear')\n",
    "\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svc.predict(X_val)\n",
    "\n",
    "accuracy_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa1c007",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(C=[0.01, 0.1, 1.0, 10, 100])\n",
    "def decision_boundaries(C=1):\n",
    "    clf = SVC(kernel='linear', C=C)\n",
    "    show_decision_regions(clf, X_train, y_train, X_val, y_val, palette, hue_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679b5775",
   "metadata": {},
   "source": [
    "## Non-linear SVM \n",
    "\n",
    "If we employ the **kernel trick** we can fit a nonlinear model. Scikit-learn's `SVC` actually uses this by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60b930b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC(C=1)  # The default kernel is 'rbf' (which is a non-linear one)\n",
    "\n",
    "svc.fit(X_val, y_val)\n",
    "\n",
    "y_pred = svc.predict(X_val)\n",
    "\n",
    "accuracy_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50ae671",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(C=[0.001, 0.001, 0.01, 0.1, 1.0, 10, 100, 1000, 1e4])\n",
    "def decision_boundaries(C=1):\n",
    "    clf = SVC(kernel='rbf', C=C)\n",
    "    show_decision_regions(clf, X_train, y_train, X_val, y_val, palette, hue_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b1d153",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "### EXERCISE:\n",
    "\n",
    "- What values of C give the highest accuracy?\n",
    "- What value(s) of C appears to yield an appropriate model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1355b5b",
   "metadata": {},
   "source": [
    "## Decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64dcd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=3)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_val)\n",
    "\n",
    "accuracy_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5d89bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlutils import lithology_tree\n",
    "\n",
    "lithology_tree(clf, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5592a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(max_depth=[2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "def decision_boundaries(max_depth=4):\n",
    "    clf = DecisionTreeClassifier(max_depth=max_depth)\n",
    "    show_decision_regions(clf, X_train, y_train, X_val, y_val, palette, hue_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba882a95",
   "metadata": {},
   "source": [
    "## Random forests and ensemble methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740bcef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=3).fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_val)\n",
    "\n",
    "accuracy_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ba4422",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(max_depth=np.arange(2, 10), min_samps_leaf=np.arange(1,6))\n",
    "def decision_boundaries(max_depth=3, min_samps_leaf=3):\n",
    "    clf = RandomForestClassifier(max_depth=max_depth, min_samples_leaf=min_samps_leaf)\n",
    "    show_decision_regions(clf, X_train, y_train, X_val, y_val, palette, hue_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb8c609",
   "metadata": {},
   "source": [
    "## Boosted trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a5fadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, ExtraTreesClassifier\n",
    "\n",
    "clf = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_val)\n",
    "\n",
    "accuracy_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7890b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(max_depth=np.arange(2, 10), min_samps_leaf=np.arange(1,6))\n",
    "def decision_boundaries(max_depth=3, min_samps_leaf=3):\n",
    "    clf = GradientBoostingClassifier(max_depth=max_depth, min_samples_leaf=min_samps_leaf)\n",
    "    show_decision_regions(clf, X_train, y_train, X_val, y_val, palette, hue_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a64d33",
   "metadata": {},
   "source": [
    "## Neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41149eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=[10, 10],\n",
    "                    learning_rate='constant',\n",
    "                    alpha=0.001,\n",
    "                    max_iter=5000,\n",
    "                    solver='adam',\n",
    "                    random_state=42,\n",
    "                   )\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_val)\n",
    "\n",
    "accuracy_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36e846b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(hl1=np.arange(2, 15, 1))\n",
    "def decision_boundaries(hl1=3):\n",
    "    clf = MLPClassifier(hidden_layer_sizes=[hl1],\n",
    "                    learning_rate='constant',\n",
    "                    alpha=0.001,\n",
    "                    max_iter=5000,\n",
    "                    solver='adam',\n",
    "                    random_state=42,\n",
    "                   )\n",
    "    show_decision_regions(clf, X_train, y_train, X_val, y_val, palette, hue_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8920306",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Choosing the right estimator\n",
    "\n",
    "Often the hardest part of solving a machine learning problem can be finding the right estimator for the job.\n",
    "\n",
    "This is a good place to start ([here](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html) is a clickable version):\n",
    "\n",
    "<img src=\"https://scikit-learn.org/stable/_static/ml_map.png\"></img>\n",
    "\n",
    "---\n",
    "\n",
    "Different estimators are better suited for different types of data and different problems. For a classifier comparison (below) check the source code [here](http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html)\n",
    "\n",
    "<img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_classifier_comparison_001.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c95a7a",
   "metadata": {},
   "source": [
    "###  [Check out this paper with a comparison of many classifiers](https://arxiv.org/abs/1708.05070)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c402e2f",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Choosing a model mean you are making an interpretation.\n",
    "- You need to know the key hyperparameters that effect how models learn.\n",
    "- KNN and SVM models have few hyperparameters.\n",
    "- Decision Trees and Neural Networks have more hyperparameters so they can be harder to \"tune\"\n",
    "- All models can be underfit and overfit to your data.\n",
    "\n",
    "## Next steps\n",
    "\n",
    "- Try some of the other classifiers that we didn't feature here.\n",
    "- Try different training and testing set sizes.\n",
    "- Swap in your own data set.\n",
    "- Beyond accuracy - Classification Reports, Confusion Matricies, ROC-AUC.\n",
    "- Beyond the visual - tuning hyperparameters in a rigorous way. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "geoml",
   "language": "python",
   "name": "geoml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
