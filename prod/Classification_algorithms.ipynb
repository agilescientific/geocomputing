{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ec33039",
   "metadata": {},
   "source": [
    "# Classification algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5846ab",
   "metadata": {},
   "source": [
    "The objectives of this notebook are to learn and compare the basic model families in shallow machine learning classication problems.\n",
    "\n",
    "We'll look at the following types of models: \n",
    "\n",
    "* Logistic regression,\n",
    "\n",
    "* Nearest neighbours models, \n",
    "    \n",
    "* Support vector machines,\n",
    "    \n",
    "* Decision Trees, and Ensemble models.\n",
    "    \n",
    "* Neural Networks (very briefly)\n",
    "\n",
    "For each model, we will:\n",
    "\n",
    "* Describe the key hyperparameters that control how these models learn.\n",
    "\n",
    "* Visualize decision boundaries study how models make predictions.\n",
    "\n",
    "* Discuss the properties of an appropriate predictive model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5351df08",
   "metadata": {},
   "source": [
    "## The dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d0dc20",
   "metadata": {},
   "source": [
    "First we'll import some data. I'm using an extract from the Rock Property Catalog, https://subsurfwiki.org/wiki/Rock_Property_Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27906f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('https://geocomp.s3.amazonaws.com/data/RPC_4_lithologies_original.csv')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69b4acb",
   "metadata": {},
   "source": [
    "We are going to drop the rows with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9b28e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9389a3",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "### EXERCISE\n",
    "\n",
    "Remove the units e.g. `[m/s]`, from any column names with units so we only need to refer to a column using its abbreviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be4234a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514445c8",
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "new_names = {'Vp [m/s]': 'Vp', 'Vs [m/s]': 'Vs', 'Rho [g/cm¬≥]': 'Rho'}\n",
    "df = df.rename(new_names, axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f201b00b",
   "metadata": {},
   "source": [
    "We'll start our discussion of classification by using the logistic regression algorithm one a variable and two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a7e713",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "Logistic regression is similar to linear regression, but instead of predicting a continuous variable, it predicts whether something is true or false. It is a classification algorithm. \n",
    "\n",
    "Instead of fitting a line to the data, Logistic regression fits a logistic function (a.k.a sigmoid) to the data. The model then is a probability function used to classify new data.\n",
    "\n",
    "$$f(x) = \\frac{1}{1+e^{-(\\textbf{wx}+b)}}$$\n",
    "\n",
    "It has many uses in data analysis and machine learning, especially in data transformations. The curve goes from zero to one. It tells you the probability that a sample is a class of interest or not. Instead of using a least-squares type loss function, it uses a maximum likelihood function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fdb75a",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "### EXERCISE\n",
    "\n",
    "- Write a function called `logistic` that takes **x**, **w**, and b as arguments and returns the function evaluated at those values.\n",
    "\n",
    "- Make a plot of the logistic function from x = -10 to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06f3a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def logistic(x, w=1, b=0):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48bb634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224ba0fb",
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def logistic(x, w=1, b=0):\n",
    "    \"\"\"Logistic function.\n",
    "    Args:\n",
    "        x (array or int): input\n",
    "        w (float or array): the weights of the logistic\n",
    "        b (float): the intercept (or bias)\n",
    "    \"\"\"\n",
    "    term = np.exp(-(w * x + b))\n",
    "    return 1 / (1 + term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0ab090",
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "x = np.arange(-10, 10)\n",
    "plt.plot(x, logistic(x), 'o-')\n",
    "plt.title('The logistic function where w=1, b=0')\n",
    "\n",
    "# Alternatively, we can use the expit function in scipy for this\n",
    "from scipy.special import expit\n",
    "plt.plot(x, logistic(x), 'o')\n",
    "plt.plot(x, expit(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca135ac",
   "metadata": {},
   "source": [
    "## Make X and y\n",
    "\n",
    "We are going to start with a 1-D feature space (Vp) and only two classes (shale and dolomite), and we'll build a logistic regression classifier to distinguish between those. Let access our DataFrame accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c02986",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Vp']  # A 1-D feature space.\n",
    "classes = ['shale', 'dolomite']  # Two classes.\n",
    "df_LR = df.loc[df['Lithology'].isin(classes)]\n",
    "\n",
    "X = df_LR[features].values\n",
    "y = df_LR['Lithology'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695339a9",
   "metadata": {},
   "source": [
    "Let's look at the data distributed along the Vp dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73ba513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Create a custom color palette for seaborn.\n",
    "colors = ['goldenrod', 'darkseagreen', 'cornflowerblue', 'blueviolet']\n",
    "palette = sns.color_palette(colors)\n",
    "hue_order = df['Lithology'].unique()\n",
    "\n",
    "_ = sns.histplot(data=df_LR, x='Vp', kde=True, hue='Lithology', palette=palette, hue_order=hue_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80074912",
   "metadata": {},
   "source": [
    "Split the data into a training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aace9097",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_lr, X_val_lr, y_train_lr, y_val_lr = train_test_split(X, y, test_size=0.2, random_state=32)\n",
    "\n",
    "X_train_lr.shape, X_val_lr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67d2818",
   "metadata": {},
   "source": [
    "## 1-D Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8084450",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "model.fit(X_train_lr, y_train_lr)\n",
    "\n",
    "y_pred_lr = model.predict(X_val_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684d691b",
   "metadata": {},
   "source": [
    "We can now use `model.predict()` to perform our classifications, but if we choose we can also take it's learned coefficients to studied the logistic curve that we fitted to the data. The sigmoid alone, however does not provide the classification directly, but it's illustrative to inspect it relative to the data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7cce97",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = model.coef_\n",
    "b = model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20dc711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our own logistic function from the learned model parameters of `model`.\n",
    "y_test_lr = logistic(X_val_lr * model.coef_ + model.intercept_)\n",
    "plt.scatter(X_val_lr, y_test_lr)\n",
    "plt.xlabel('Vp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c95e2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "from mlutils import logistic_progression\n",
    "\n",
    "@interact(cutoff=np.arange(0, 1.0, 0.05))\n",
    "def logistic_regression_plot(cutoff=0.5):\n",
    "    logistic_progression(model, X_val_lr, y_val_lr, y_test_lr, cutoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5d2eee",
   "metadata": {},
   "source": [
    "## Adding more features and classes\n",
    "\n",
    "Let's take a look at all the rows in `df` for the four classes in `Vp` vs `Rho` space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f16a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter = sns.relplot(data=df, x='Vp', y='Rho', hue='Lithology', s=80, alpha=0.5, height=6, \n",
    "                      palette=palette, hue_order=hue_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225e0b12",
   "metadata": {},
   "source": [
    "## Make X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3460f6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Vp', 'Rho']  # A 2-D feature space. \n",
    "X = df[features].values  # Including all four classes.\n",
    "y = df['Lithology']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5071be27",
   "metadata": {},
   "source": [
    "## Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbac3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e075f044",
   "metadata": {},
   "source": [
    "## Standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922e3db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "\n",
    "X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52f6c55",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4813d0ea",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "### EXERCISE\n",
    "\n",
    "- Import the logistic regression classifier, again for good practice.\n",
    "\n",
    "- Instantiate the classifier and assign it to the variable `model`.\n",
    "\n",
    "- Train the model on the training data\n",
    "\n",
    "- Make a prediction using the validation data.\n",
    "\n",
    "Your code should look like, replacing the üêçüêçüêç with Python code:\n",
    "\n",
    "```\n",
    "from üêçüêçüêç import üêçüêçüêç\n",
    "\n",
    "model = üêçüêçüêç\n",
    "model.fit(üêçüêçüêç, üêçüêçüêç)\n",
    "y_pred = model.predict(üêçüêçüêç)\n",
    "```\n",
    "\n",
    "If you've done this correctly the plot in the following cell will show the predicted label inside the true label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a536101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ae4542",
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b32c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from mlutils import val_vs_pred_scatter\n",
    "\n",
    "print(f'Accuracy: {accuracy_score(y_val, y_pred):0.3f}')\n",
    "val_vs_pred_scatter(X_val, y_val, y_pred, palette, hue_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f053043b",
   "metadata": {},
   "source": [
    "## Plotting the decision regions\n",
    "\n",
    "The cutoff we saw in the one-dimensional logistic regression is expressed as decision boundaries in 2-D. We can visualize these decision regions for our problem using the following function. Note, we aren't showing the training data in these plots, only showing the validation and predicted data points, other wise it would look very messy. But the influence of the training data can be seem in the shape of the decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36456c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlutils import show_decision_regions\n",
    "\n",
    "show_decision_regions(model, X_train, y_train, X_val, y_val, palette, hue_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e251d1",
   "metadata": {},
   "source": [
    "## K-Nearest neighbours\n",
    "\n",
    "K-Nearest neighbours (kNN) is a supervised learning algorithm that tries to classify a test point by calculating the probability of each of the classes within the `k` closest datapoints. For each test data point, this is done by calculating the distance between the test point and all the training points. The `k` closest points (neighbors) are selected and are their classes are counted as votes to make a class prediction. kNN is called a non-parameteric classifier because it doesn't learn any parameters from the training. It learns no simpler expression of the problem, it merely calculates and recalculates distances to all points in the training data everytime a test point is passed in?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89a847f",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "### EXERCISE\n",
    "\n",
    "- Create and train a `KNeighborsClassifier` using its default parameters in a similar way you did for LogisticRegression. You might need to search the scikit-learn website to find what module to import it from.\n",
    "\n",
    "- Make a new prediction, called `y_pred` from your validation data.\n",
    "\n",
    "- compute the accuracy score.\n",
    "\n",
    "\n",
    "Your code should look like, replacing the üêçüêçüêç with Python code:\n",
    "\n",
    "```\n",
    "from üêçüêçüêç import üêçüêçüêç\n",
    "\n",
    "clf = KNeighborsClassifier()  \n",
    "clf.fit(üêçüêçüêç, üêçüêçüêç)\n",
    "y_pred = clf.predict(üêçüêçüêç)\n",
    "accuracy_score(üêçüêçüêç, üêçüêçüêç)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69cb513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07851c44",
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf = KNeighborsClassifier()  \n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "accuracy_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89d63c4",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "### EXERCISE:\n",
    "\n",
    "Let's look at how the decision boundaries and accuracy changes for different values of k.\n",
    "\n",
    "- Do high or low values of `k` create a smoothing effect?\n",
    "- For `k=100` how many times is sandstone being mistaken as shale?\n",
    "- What value of `k` gives the highest accuracy?\n",
    "- Does this model look like a good choice for our data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e11df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "\n",
    "@interact(k=[1, 3, 5, 10, 20, 30, 50, 100, 150])\n",
    "def decision_boundaries(k=5):\n",
    "    clf = KNeighborsClassifier(n_neighbors=k)\n",
    "    show_decision_regions(clf, X_train, y_train, X_val, y_val, palette, hue_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d35514",
   "metadata": {},
   "source": [
    "## Support-vector machine (SVM)\n",
    "\n",
    "The objective of the support vector machine alogorithm is to find a **hyperplane** that distinctly separates the classes in the feature space. \n",
    "\n",
    "So, for a one-dimensional feature space, a hyperplane is a dot. In a two-dimesional feature space, a hyperplane is a line. In a three-dimensional feature space a hyperplane is a plane. In higher-dimensional space a hyperplane is still called a hyperplane.\n",
    "\n",
    "**Support vectors** are those data points that are closest to the hyperplanes and therefore influence the position and orientation of the decision boundary. If you delete a support vector, you will move the decision boundary. If you remove any point that isn't a support vector you will not change the boundary. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff8ed07",
   "metadata": {},
   "source": [
    "## Linear SVM\n",
    "\n",
    "### Hard margin\n",
    "\n",
    "If the training data is linearly seperable, we can select two parallel hyperplanes that separate the two classes of data, so that the distance between them is as large as possible. The region bounded by these two hyperplanes is called the \"margin\", and the decision boundary is defined to be the hyperplane that lies halfway in between.\n",
    "\n",
    "### Soft margin\n",
    "\n",
    "There are is no line that can perfectly separate the data, then there must be a tradeoff to be made. the tradeoff is between maximizing the margin and minimizing the error or being on the wrong side of the line (loss). \n",
    "\n",
    "The regularization parameter `C` controls this trade-off between margin expansion and loss reduction. Large values of `C` mean you want to minimize misclassification, and have a small margin. Small values of `C` mean you want to maintain a large margin at the expense of higher misclassification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470a42f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC(kernel='linear')\n",
    "\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svc.predict(X_val)\n",
    "\n",
    "accuracy_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa1c007",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(C=[0.005, 0.05, 0.1, 0.5, 1.0, 10, 100])\n",
    "def decision_boundaries(C=1):\n",
    "    clf = SVC(kernel='linear', C=C)\n",
    "    show_decision_regions(clf, X_train, y_train, X_val, y_val, palette, hue_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679b5775",
   "metadata": {},
   "source": [
    "## Non-linear SVM \n",
    "\n",
    "If we are not satified with the linear classification shown above, we may try a non-linear SVM method. The Non-linear SVM method effectively maps our data onto higher-dimensions than our original feature space, which makes it easier to find a hyperplane to separate our classes. \n",
    "\n",
    "However, instead of explicitly mapping our data points into a potentially huge and costly number of dimensions, non-linear SVM classifiers employ the so called **kernel trick** which applies nonlinear functions between points. \n",
    "\n",
    "There are several non-linear kernel functions that are used, but the `rbf` (radial basis function) kernel, which applies a gaussian function, is popular, and it is what Scikit-learn uses by default.\n",
    "\n",
    "The key hyperparamters to consider when training with the `rbf` kernel is `C` and `gamma`:\n",
    "\n",
    "* `C` trades off missclassification of the training data against the simpliciity of the decision boundary. A low `C` makes for smooth boundaries, a high `C` aims at classifying more points correctly and results in a more complicated boundaries. \n",
    "\n",
    "* `gamma` defines how far the influence of a single training example has. Low values of `gamma` mean 'far', and high values mean 'close'. The `gamma` parameter can be seen as the inverse of the radius of influence of samples selected by the model as support vectors\n",
    "\n",
    "Here's an [example](https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html#sphx-glr-auto-examples-svm-plot-rbf-parameters-py) in the Scikit-learn documentation comparing these two hyperparameters;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60b930b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC(C=1)  # The default kernel is 'rbf' (which is a non-linear one).\n",
    "\n",
    "svc.fit(X_val, y_val)\n",
    "\n",
    "y_pred = svc.predict(X_val)\n",
    "\n",
    "accuracy_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50ae671",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(C=[0.001, 0.01, 0.1, 1.0, 10, 100, 1000, 1e4, 1e8])\n",
    "def decision_boundaries(C=1):\n",
    "    clf = SVC(kernel='rbf', C=C, gamma=10)\n",
    "    show_decision_regions(clf, X_train, y_train, X_val, y_val, palette, hue_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b1d153",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "### EXERCISE:\n",
    "\n",
    "- What value of `C` returns the highest accuracy?\n",
    "- Choose a value of `C` that appears to yield an appropriate model.\n",
    "- Study the effect of `gamma` using the following values: `0.1, 1, 10, 100`.\n",
    "- What combination of `C` and `gamma` is a good fit?\n",
    "- How does this compare to kNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1355b5b",
   "metadata": {},
   "source": [
    "## Decision trees\n",
    "\n",
    "[Decision trees](https://scikit-learn.org/stable/modules/tree.html) are a supervised learning method whereby the goal is to predict the class of a target variable by learning simple decision rules inferred from the data features. \n",
    "\n",
    "The structure of a decision is a set of `if-then-else` rules. Each decision asks an `if` statement of one and only one of the features at a time. The deeper the tree, the more complex the decision rules become and the fitter the model. If decision trees are too deep, they are prone to overfitting the training data.\n",
    "\n",
    "One recourse against overfitting is to limit the `max_depth` of the tree. The `max_depth` is maximum the number of `if` statements a data point will be asked. Another way to avoid overfitting is to increase the `min_samples_leaf`, minimum samples per leaf, to ensure that multiple samples will inform every decision in the tree. A decision that is informed by a very small number training points is not a decision fit for general purpose (overfit). \n",
    "\n",
    "Try `min_samples_leaf` greater than or equal to 5. Try `max_depth`= 3, see how it fits the data, and then increase from there.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64dcd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=3)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_val)\n",
    "\n",
    "accuracy_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5d89bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlutils import lithology_tree\n",
    "\n",
    "lithology_tree(clf, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5592a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(max_depth=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 50])\n",
    "def decision_boundaries(max_depth=1):\n",
    "    clf = DecisionTreeClassifier(max_depth=max_depth, min_samples_leaf=5)\n",
    "    show_decision_regions(clf, X_train, y_train, X_val, y_val, palette, hue_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f3e0fb",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "### EXERCISE\n",
    "\n",
    "- Study the effect of `max_depth` on the decision boundaries.\n",
    "- At what depth does the decision surface stay approximately the same.\n",
    "- Increase `min_samples_leaf` and notice the effect at different values of `max_depth`\n",
    "- Can you choose two values that give the highest accuracy score?\n",
    "- Does this model look like a good choice for our problem?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba882a95",
   "metadata": {},
   "source": [
    "## Random forests and ensemble methods\n",
    "\n",
    "Random forests is type of ensemble methods combine many weak learners together, in this case decision trees, to create a stronger predictive model.  \n",
    "\n",
    "For a classification tasks, the output of the random forest is the class chosen by the most number of decision trees.\n",
    "\n",
    "The training algorithm for random forests applies two techniques that help reduce overfitting and create an ensembles of effective weak learners. The way to achieve this is to create diversity in the forest. This is done by only showing **some** of the data to each tree, and no two trees ever see the same data. \n",
    "\n",
    "The data resampling procedure is used for this is called [bootstrap aggregating](https://en.wikipedia.org/wiki/Bootstrap_aggregating) or \"Bagging\".\n",
    "\n",
    "The key hyperparameters for the RandomForestClassifier are the same as the decision tree, but also the number of trees `n_estimators` which has the default of 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740bcef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=3).fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_val)\n",
    "\n",
    "accuracy_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ba4422",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(n_estimators=[1, 5, 10, 20, 50, 100, 150] ,\n",
    "          max_depth=np.arange(2, 10), \n",
    "          min_samps_leaf=np.arange(1,6))\n",
    "def decision_boundaries(n_estimators, max_depth=3, min_samps_leaf=3):\n",
    "    clf = RandomForestClassifier(n_estimators, max_depth=max_depth, min_samples_leaf=min_samps_leaf)\n",
    "    show_decision_regions(clf, X_train, y_train, X_val, y_val, palette, hue_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2010ee",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "### EXERCISE\n",
    "\n",
    "- Study the effect of `n_estimators` on the output.\n",
    "- Adjust the parameters to see how they trade off between one another.\n",
    "- Set `n_estimators` to `100` and `min_samples_leaf` to `3`. What value of `max_depth` gives the highest accuracy?\n",
    "- Bonus: read about [sklearn.ensemble.ExtraTreesClassifier()](https://scikit-learn.org/stable/modules/ensemble.html#forest) and try implementing that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb8c609",
   "metadata": {},
   "source": [
    "## Boosted trees\n",
    "\n",
    "[Gradient boosted decision trees (GBDT)](https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting) are similar to Random Forests in the sense they are comprised on an ensemble of trees each of which is a weak learner, but they differ in the way that the trees are built. Random Forests use `bagging` as described above, whereas gradient boosted trees use a method called `boosting`.\n",
    "\n",
    "How gradient boosting works\n",
    "\n",
    "1. A loss function to be optimized\n",
    "2. A weak learner to make predictions\n",
    "3. An additive model to add more weak learners to mimimize the loss function.\n",
    "\n",
    "With Random Forests, all trees are created simultaneously. With Gradient Boosting, trees are added one at a time, and existing trees in the model are not modified. \n",
    "\n",
    "Using a loss function, a gradient descent procedure is used to minimize the loss when new trees are added.\n",
    "\n",
    "The key hyperparameters are the same as for Random Forests, except `n_estimators` refers to the number of boosting stages to perform (which amounts to the same as the total number of trees).\n",
    "\n",
    "The two most important parameters of this estimator are `n_estimators` and `learning_rate` shrinks the contribution of each tree.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a5fadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "clf = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_val)\n",
    "\n",
    "accuracy_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7890b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(n_estimators=np.arange(1, 150, 10), learning_rate=[0.001, 0.01, 0.1, 0.2, 0.3, 0.4])\n",
    "def decision_boundaries(n_estimators=20, learning_rate=0.1):\n",
    "    clf = GradientBoostingClassifier(n_estimators=n_estimators, learning_rate=learning_rate)\n",
    "    show_decision_regions(clf, X_train, y_train, X_val, y_val, palette, hue_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f93390",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "### EXERCISE\n",
    "\n",
    "- Fix the learning rate at 0.1 in the plot above. What is the value of `n_estimators` where there is no improvement in accuracy.\n",
    "- What is happening for any value of `n_estimators` above this number?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a64d33",
   "metadata": {},
   "source": [
    "## Neural networks\n",
    "\n",
    "Neural networks are useful for a variety supervised machine learning classification tasks. The neural network classifier available in scikit-learn is called a Multi-layer Perceptron classifier. \n",
    "\n",
    "Neural networks can be powerful classifiers indeed, but there are many hyperparameters to choose from, which means you typically need a very large amount of data, and compute time to find the optimal ones. \n",
    "\n",
    "We will include the `MLPClassifier here` for completeness, but won't delve into the terminology comprehensively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41149eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=[10, 10])\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_val)\n",
    "\n",
    "accuracy_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36e846b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(hl1=np.arange(2, 15, 1))\n",
    "def decision_boundaries(hl1=3):\n",
    "    clf = MLPClassifier(hidden_layer_sizes=[hl1],\n",
    "                    learning_rate='constant',\n",
    "                    alpha=0.001,\n",
    "                    max_iter=5000,\n",
    "                    solver='adam',\n",
    "                    random_state=42,\n",
    "                   )\n",
    "    show_decision_regions(clf, X_train, y_train, X_val, y_val, palette, hue_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abcacf0",
   "metadata": {},
   "source": [
    "### EXERCISE\n",
    "\n",
    "- What effect does increasing the number of nodes in the hidden layer `hl1` of the network have?\n",
    "- Trying adding a second number to the list of `hidden_layer_sizes=[hl1]` to add more nodes. Does this add or remove complexity\n",
    "- How does this classifier compare to the others we've seen?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8920306",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Choosing the right estimator\n",
    "\n",
    "Often the hardest part of solving a machine learning problem can be finding the right estimator for the job.\n",
    "\n",
    "This is a good place to start ([here](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html) is a clickable version):\n",
    "\n",
    "<img src=\"https://scikit-learn.org/stable/_static/ml_map.png\"></img>\n",
    "\n",
    "---\n",
    "\n",
    "Different estimators are better suited for different types of data and different problems. For a classifier comparison (below) check the source code [here](http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html)\n",
    "\n",
    "<img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_classifier_comparison_001.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c95a7a",
   "metadata": {},
   "source": [
    "###  [Check out this paper with a comparison of many classifiers](https://arxiv.org/abs/1708.05070)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c402e2f",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Choosing a model mean you are making an interpretation.\n",
    "- You need to know the key hyperparameters that effect how models learn.\n",
    "- KNN and SVM models have few hyperparameters.\n",
    "- Decision Trees and Neural Networks have more hyperparameters so they can be harder to \"tune\"\n",
    "- All models can be underfit and overfit to your data.\n",
    "\n",
    "## Next steps\n",
    "\n",
    "- Try some of the other classifiers that we didn't feature here.\n",
    "- Try different training and testing set sizes.\n",
    "- Swap in your own data set.\n",
    "- Beyond accuracy - Classification Reports, Confusion Matricies, ROC-AUC.\n",
    "- Beyond the visual - tuning hyperparameters in a rigorous way. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "geoml",
   "language": "python",
   "name": "geoml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
