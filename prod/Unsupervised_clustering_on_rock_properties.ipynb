{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised learning on rock properties\n",
    "\n",
    "Sometimes we don't have labels, but would like to discover structure in a dataset. This is what clustering algorithms attempt to do. They don't require labels from us &mdash; they are 'unsupervised'.\n",
    "\n",
    "We'll use a subset of the [Rock Property Catalog](http://subsurfwiki.org/wiki/Rock_Property_Catalog) data, licensed CC-BY Agile Scientific. Note that the data have been preprocessed, including the addition of noise to the density data.\n",
    "\n",
    "We'll use three unsupervised clustering techniques:\n",
    "\n",
    "- k-means clustering\n",
    "- DBSCAN\n",
    "- HDBSCAN\n",
    "\n",
    "And two unsupervised dimensionality reduction techniques:\n",
    "\n",
    "- t-SNE\n",
    "- UMAP\n",
    "\n",
    "We do have lithology labels for this dataset (normally in an unsupervised problem you wouldn't have labels), so we can use those as a measure of how well we're doing with the clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://geocomp.s3.amazonaws.com/data/RPC_4_lithologies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the count of `Rho` values is smaller than for the other properties.\n",
    "\n",
    "Pairplots are a good way to see how the various features are distributed with respect to each other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Vp', 'Vs', 'Rho_n']\n",
    "\n",
    "sns.pairplot(df.dropna(), vars=features, hue='Lithology')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering with _k_-means\n",
    "\n",
    "From [the Wikipedia article](https://en.wikipedia.org/wiki/K-means_clustering):\n",
    "\n",
    "> k-means clustering is a method of vector quantization, originally from signal processing, that is popular for cluster analysis in data mining. k-means clustering aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clu = KMeans()\n",
    "\n",
    "# This will fail...\n",
    "clu.fit(df[features].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The old classic: NaNs. Remember the count of `Rho` points being smaller than the others?\n",
    "\n",
    "The easiest thing to do, assuming we have the data, is to drop the rows with NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clu.fit(df[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Kmeans'] = clu.predict(df[features].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, group in df.groupby('Kmeans'):\n",
    "    plt.scatter(group.Vp, group.Rho_n, label=name)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We actually do have the labels, so let's compare..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, group in df.groupby('Lithology'):\n",
    "    plt.scatter(group.Vp, group.Rho_n, label=name)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clu = KMeans(n_clusters=4)\n",
    "df['K means'] = clu.fit_predict(df[features])\n",
    "for name, group in df.groupby('K means'):\n",
    "    plt.scatter(group.Vp, group.Rho_n, label=name)\n",
    "plt.legend()\n",
    "\n",
    "# We can add the centroids as well:\n",
    "plt.plot(*clu.cluster_centers_[:, ::2].T, 'o', c='k', ms=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering with DBSCAN\n",
    "\n",
    "DBSCAN has nothing to do with databases. From [the Wikipedia article](https://en.wikipedia.org/wiki/DBSCAN):\n",
    "\n",
    "> Density-based spatial clustering of applications with noise (DBSCAN) is [...] a density-based clustering algorithm: given a set of points in some space, it groups together points that are closely packed together (points with many nearby neighbors), marking as outliers points that lie alone in low-density regions (whose nearest neighbors are too far away). DBSCAN is one of the most common clustering algorithms and also most cited in scientific literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "DBSCAN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two important hyperparameters:\n",
    "\n",
    "- `eps`, the maximum distance between points in the same cluster.\n",
    "- `min_samples`, the minimum number of samples in a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clu = DBSCAN(eps=150, min_samples=10)\n",
    "\n",
    "clu.fit(df[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DBSCAN'] = clu.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, group in df.groupby('DBSCAN'):\n",
    "    plt.scatter(group.Vp, group.Rho_n, label=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a bit hard to juggle the two parameters... let's make an interactive widget:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "\n",
    "@interact(eps=(10, 250, 10))\n",
    "def plot(eps):\n",
    "    clu = DBSCAN(eps=eps)\n",
    "    clu.fit(df[features])\n",
    "    df['DBSCAN'] = clu.labels_\n",
    "    for name, group in df.groupby('DBSCAN'):\n",
    "        plt.scatter(group.Vp, group.Rho_n, label=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDBSCAN\n",
    "\n",
    "### Clustering with HDBSCAN\n",
    "\n",
    "HDBSCAN is an improvement on DBSCAN, but is not yet available in `sklearn`. Install it with:\n",
    "\n",
    "    conda install -c conda-forge hdbscan\n",
    "\n",
    "DBSCAN has nothing to do with databases. From [the Wikipedia article](https://en.wikipedia.org/wiki/DBSCAN):\n",
    "\n",
    "> Density-based spatial clustering of applications with noise (DBSCAN) is [...] a density-based clustering algorithm: given a set of points in some space, it groups together points that are closely packed together (points with many nearby neighbors), marking as outliers points that lie alone in low-density regions (whose nearest neighbors are too far away). DBSCAN is one of the most common clustering algorithms and also most cited in scientific literature.\n",
    "\n",
    "HDBSCAN does away with the epsilon parameter (the maximum distance between points in the same cluster) in DBSCAN, leaving only `n`. So there is only one important hyperparameter:\n",
    "\n",
    "- `min_samples`, the minimum number of samples in a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdbscan import HDBSCAN\n",
    "\n",
    "clu = HDBSCAN(min_samples=10)\n",
    "\n",
    "clu.fit(df[features])\n",
    "\n",
    "df['HDBSCAN'] = clu.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anything the algothim considers to be noise is assigned -1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(clu.labels_)  # -1 is 'noise'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, group in df.groupby('HDBSCAN'):\n",
    "    plt.scatter(group.Vp, group.Rho_n, label=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "\n",
    "@interact(min_samples=(1, 60, 2))\n",
    "def plot(min_samples):\n",
    "    clu = HDBSCAN(min_samples=min_samples)\n",
    "    clu.fit(df[features])\n",
    "    df['HDBSCAN'] = clu.labels_\n",
    "    for name, group in df.groupby('HDBSCAN'):\n",
    "        plt.scatter(group.Vp, group.Rho_n, label=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing clusterings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are metrics for comparing clusterings. For example, `adjusted_rand_score` &mdash; from the scikit-learn docs:\n",
    "\n",
    "> The Rand Index computes a similarity measure between two clusterings by considering all pairs of samples and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings.\n",
    ">\n",
    "> The raw RI score is then “adjusted for chance” into the ARI score using the following scheme:\n",
    "> \n",
    "> ARI = (RI - Expected_RI) / (max(RI) - Expected_RI)\n",
    "> \n",
    "> The adjusted Rand index is thus ensured to have a value close to 0.0 for random labeling independently of the number of clusters and samples and exactly 1.0 when the clusterings are identical (up to a permutation).\n",
    "\n",
    "Conveniently, there is no need for the labels to correspond &mdash; the algorithm just compares whether similar points in one clustering are still similar in the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "print(adjusted_rand_score(df.Lithology, df.Kmeans))\n",
    "print(adjusted_rand_score(df.Lithology, df.DBSCAN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of other clustering algorithms to try. This figure is from [the `sklearn` docs](https://scikit-learn.org/stable/modules/clustering.html):\n",
    "\n",
    "<img src=\"../images/sphx_glr_plot_cluster_comparison_0011.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manifold embedding with t-SNE\n",
    "\n",
    "[`t-SNE`](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) was recently implemented in `sklearn`. t-statistic neighbourhood embedding is a popular and very effective dimensionality reduction strategy. The caveat is that distance between clusters is not typically meaningful. But it's at least a useful data exploration tool.\n",
    "\n",
    "Usually we want `n_components=2`, so we get the data into a 2-space, e.g. for cross-plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(init='random', perplexity=100)\n",
    "\n",
    "embedding = tsne.fit_transform(df[features].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "labels = LabelEncoder().fit_transform(df.Lithology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(*embedding.T, c=labels, cmap='tab10', vmax=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manifolds with UMAP\n",
    "\n",
    "Another popular tool is UMAP. Check out [`umap-learn`](https://pypi.org/project/umap-learn/). You can install it with \n",
    "\n",
    "    conda install -y -c conda-forge umap-learn\n",
    "    \n",
    "It has the same interface as `sklearn` so it's very easy to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "\n",
    "umap = UMAP(metric='euclidean')\n",
    "\n",
    "embed_umap = umap.fit_transform(df[features].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(*embed_umap.T, c=labels, cmap='tab10', vmax=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's fairly common to attempt clustering on the result of the dimensionality reduction, but you should be wary of treating the embedding as a metric space. That is, distances in this space may not correspond to any natural or useful property."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "### Exercises\n",
    "\n",
    "- Can you make the interactive widget display the Rand score? Use `plt.text(x, y, \"Text\")`.\n",
    "- Can you write a loop to find the value of `eps` giving the highest Rand score?\n",
    "- Can you add the `min_samples` parameter to the widget?\n",
    "- Explore some of [the other clustering algorithms](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.cluster).\n",
    "- Try some clustering on one of your own datasets (or use something from [sklearn](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets), e.g. `sklearn.datasets.load_iris`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "geoml",
   "language": "python",
   "name": "geoml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
