{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo simulation of rock properties\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily draw randomly from distributions of properties:\n",
    "\n",
    "- Normal: https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.normal.html\n",
    "- Uniform: https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.uniform.html\n",
    "- Lognormal: https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.lognormal.html\n",
    "\n",
    "The normal distribution is probably familiar:\n",
    "\n",
    "<img src=\"https://subsurfwiki.org/images/3/3a/Normal_distribution.png\" width=\"500px\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "rho = np.random.normal(loc=2500, scale=125, size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_ = plt.hist(rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "sns.displot(rho, rug=True, kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same thing in `scipy`\n",
    "\n",
    "There are continuous (and discrete) distributions in `scipy` too. There are more of them ([a lot more!](https://docs.scipy.org/doc/scipy/reference/tutorial/stats/continuous.html#continuous-distributions-in-scipy-stats)), and they allow a bit more flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "\n",
    "mean = 3   # aka mu or loc\n",
    "stdev = 2  # aka sigma or scale\n",
    "normal_distribution = st.norm(loc=mean, scale=stdev)\n",
    "\n",
    "x = np.linspace(-6, 12, 200)\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "ax.plot(x, normal_distribution.pdf(x), '-', lw=2)\n",
    "plt.title(f'Normal distribution: mean = {mean}, stdev = {stdev}')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_rho = st.norm(loc=2500, scale=125)\n",
    "d_rho.rvs()  # Random variates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho = d_rho.rvs(size=200, random_state=42)\n",
    "\n",
    "sns.displot(rho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a distribution\n",
    "\n",
    "Using the Rock Property Catalog: https://subsurfwiki.org/wiki/Rock_Property_Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('https://geocomp.s3.amazonaws.com/data/RPC_4_lithologies.csv')\n",
    "df = df.dropna()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE\n",
    "\n",
    "The original density data (`df.Rho`) were discretized in the original lab measurements. I've added some random noise to these values to get a more natural distribution. So we'll use `df.Rho_n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, group in df.groupby('Lithology'):\n",
    "    plt.scatter(group.Vp, group.Rho_n, label=name)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, group in df.groupby('Lithology'):\n",
    "    sns.kdeplot(group.Vp, label=name)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit a normal distribution to this data. We'll focus on limestone first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "limestone = df.loc[df.Lithology=='limestone']\n",
    "\n",
    "loc, scale = st.norm.fit(limestone.Vp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc, scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the learned distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_vp = st.norm(loc=loc, scale=scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare its PDF to the actual distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(1500, 5000, 1000)\n",
    "\n",
    "sns.kdeplot(limestone.Vp)\n",
    "plt.plot(x, l_vp.pdf(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe a skewed distribution, like `gumbel_r` (`r` for 'right') distribution is better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc, scale = st.gumbel_r.fit(limestone.Vp)\n",
    "\n",
    "l_vp = st.gumbel_r(loc=loc, scale=scale)\n",
    "\n",
    "sns.kdeplot(limestone.Vp)\n",
    "plt.plot(x, l_vp.pdf(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample from the distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp = l_vp.rvs(size=200)\n",
    "\n",
    "sns.displot(vp, rug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "### EXERCISE\n",
    "\n",
    "Repeat this exercise for the `df.Rho_n` data. You should end up with an array of 200 samples drawn from the distribution.\n",
    "\n",
    "If there's time, check some other distributions in `scipy.stats`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "# Check the distribution:\n",
    "sns.kdeplot(limestone.Rho_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "loc, scale = st.norm.fit(limestone.Rho_n)\n",
    "l_rho = st.norm(loc=loc, scale=scale)\n",
    "\n",
    "# Do in one step:\n",
    "#   d_rho = norm(*norm.fit(dolomite.Rho_n))\n",
    "\n",
    "rho = l_rho.rvs(size=200)\n",
    "\n",
    "sns.displot(rho, kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel density estimation\n",
    "\n",
    "It's possible to model 'lumpy' distributions as 'mixtures of Gaussians', the most general expression of which is the kernel density estimate. \n",
    "\n",
    "Let's model the limestone's `Rho_n` distribution that way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(limestone.Rho_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, there's no easy way to get the KDE that Seaborn is plotting. We'll have to compute it ourselves.\n",
    "\n",
    "We could use `scipy.stats.gaussian_kde()` but it doesn't have a way to generate ranvom variates, whereas `KernelDensity` in `sklearn.neighbors` comes with a `sample()` method, so let's use that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "X = np.array(limestone.Rho_n).reshape(-1, 1)  # X must be 2D.\n",
    "kde = KernelDensity(kernel='gaussian', bandwidth=0.5).fit(X)\n",
    "sample = np.squeeze(kde.sample(n_samples=500))\n",
    "\n",
    "_ = plt.hist(sample, bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint distributions\n",
    "\n",
    "Sometimes variables vary together, so we can't draw them separately.\n",
    "\n",
    "Let's simulate impedance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impedance(vp, rho):\n",
    "    return vp * rho\n",
    "\n",
    "imp = impedance(vp, rho)\n",
    "\n",
    "imp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(imp, kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the joint distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x=vp, y=rho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And compare this to the actual data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x=dolomite.Vp, y=dolomite.Rho_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll have to do something about that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Gaussian simulation\n",
    "\n",
    "If the two distributions are Gaussian, then we can use a multivariate Gaussian distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(df[['Vp', 'Rho_n']])\n",
    "cov = np.cov(df[['Vp', 'Rho_n']], rowvar=False)  # vars are in columns.\n",
    "\n",
    "multi = st.multivariate_normal(mean=mean, cov=cov, seed=42)\n",
    "\n",
    "samples = multi.rvs(size=200, random_state=42)\n",
    "\n",
    "samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp, rho = samples.T\n",
    "\n",
    "sns.jointplot(x=vp, y=rho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem solved!\n",
    "\n",
    "But... this multivariate Gaussian simulation only works on normally distributed variables. What if our variables do not fit normal distributions? \n",
    "\n",
    "We will need to transform them to Gaussians, simulate them, then back-transform them to their original distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if the marginal distributions are not Gaussian?\n",
    "\n",
    "So we have a way to model normal distributions; but what if your variables are better modeled by other distributions? \n",
    "\n",
    "We can transform random variables to and from a uniform distribution using the [probabiity integral transform](https://en.wikipedia.org/wiki/Probability_integral_transform). From Wikipedia: \n",
    "\n",
    "> [...] the probability integral transform (also known as universality of the uniform) relates to the result that data values that are modeled as being random variables from any given continuous distribution can be converted to random variables having a standard uniform distribution. This holds exactly provided that the distribution being used is the true distribution of the random variables; if the distribution is one fitted to the data, the result will hold approximately in large samples.\n",
    "\n",
    "I'm following [this blog post](https://twiecki.io/blog/2018/05/03/copulas/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a uniform sampling on [0, 1]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = st.uniform(0, 1).rvs(10000)\n",
    "sns.displot(x, kde=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can transform these samples to a normal distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = st.norm()\n",
    "x_trans = norm.ppf(x)\n",
    "sns.displot(x_trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the inverse CDF given by the model's `ppf()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = sns.jointplot(x=x, y=x_trans)\n",
    "h.set_axis_labels('original', 'transformed', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this with any distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gumbel = st.gumbel_l()\n",
    "x_trans = gumbel.ppf(x)\n",
    "h = sns.jointplot(x=x, y=x_trans)\n",
    "h.set_axis_labels('original', 'transformed', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To go backwards, we apply the CDF (the inverse of the inverse CDF!)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trans_trans = gumbel.cdf(x_trans)\n",
    "h = sns.jointplot(x=x_trans, y=x_trans_trans)\n",
    "h.set_axis_labels('original', 'transformed', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means we can convert a Gumbel (say) to a Gaussian:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we have to make the Gumbel we're pretending we're starting from:\n",
    "gumbel = st.gumbel_l()\n",
    "x_gumbel = gumbel.ppf(x)\n",
    "\n",
    "# Now transform to uniform, then Gaussian:\n",
    "x_gumbel_uniform = gumbel.cdf(x_trans)\n",
    "\n",
    "norm = st.norm()\n",
    "x_uniform_normal = norm.ppf(x_gumbel_uniform)\n",
    "\n",
    "# And plot:\n",
    "h = sns.jointplot(x=x_gumbel, y=x_uniform_normal)\n",
    "h.set_axis_labels('original', 'transformed', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why would we do this? From [a blog post by Pyrcz and Deutsch](http://www.geostatisticslessons.com/lessons/normalscore)\n",
    "\n",
    "> Modern geostatistical algorithms and software all invoke the multivariate Gaussian (MG) distribution for probabilistic prediction of continuous properties. A requirement of the MG distribution is that the univariate distribution must be Gaussian. The procedure developed early on in multivariate statistics and adopted by geostatistics is to: (1) transform the data to a univariate Gaussian distribution, (2) proceed with algorithms that take advantage of the properties of the multivariate Gaussian distribution, then (3) back transform results to original units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some porosity data. We'll use data from Atkinson, CD, JH McGowen, S Bloch, LL Lundell, and PN Trumbly, 1990, Braidplain and deltaic reservoir, Prudhoe Bay, Alaska, in JH Barwis, JG McPherson, and RJ Studlick, eds, Sandstone petroleum reservoirs: New York, Springer-Verlag, p 7–29."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read a file from Google Sheets:\n",
    "uid = \"1QcSw_xRAYgJzD9HsIXNjmS7o4Zb6qkRBgIWhmp4f2mI\"\n",
    "url = f\"https://docs.google.com/spreadsheets/d/{uid}/export?format=csv\"\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "for name, group in df.groupby('Gross environment'):\n",
    "    plt.scatter(group.Porosity, np.log10(group.Permeability), label=name)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaic = df.loc[df['Gross environment']=='Deltaic']\n",
    "\n",
    "sns.displot(deltaic['Porosity'], kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "### EXERCISE\n",
    "\n",
    "Can you create a KDE of this porosity data and draw 1000 samples from it?\n",
    "\n",
    "Can you create a joint poro-perm distribution for the Deltaic environment, and draw 1000 samples from that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "# Solution!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "source": [
    "## All the distributions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "source": [
    "You could even compute all the distributions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "# From: https://stackoverflow.com/a/37616966/3381305\n",
    "# (c): User stackoverflow.com/users/2087463/tmthydvnprt\n",
    "# Licensed: CC-BY-SA 4.0\n",
    "\n",
    "import warnings\n",
    "import matplotlib\n",
    "from scipy.stats._continuous_distns import _distn_names\n",
    "from tqdm import tqdm\n",
    "\n",
    "def best_fit_distribution(data, bins=100, ax=None, skip=None):\n",
    "    \"\"\"Model data by finding best fit distribution to data\"\"\"\n",
    "    y, x = np.histogram(data, bins=bins, density=True)\n",
    "    x = (x + np.roll(x, -1))[:-1] / 2.0\n",
    "    \n",
    "    if skip is None:\n",
    "        skip = []\n",
    "\n",
    "    DISTRIBUTIONS = [getattr(st, d) for d in _distn_names if d not in skip]\n",
    "\n",
    "    best_distribution = st.norm\n",
    "    best_params = (0.0, 1.0)\n",
    "    best_sse = np.inf\n",
    "\n",
    "    for distribution in tqdm(DISTRIBUTIONS):\n",
    "\n",
    "        try:\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.filterwarnings('ignore')\n",
    "\n",
    "                params = distribution.fit(data)\n",
    "                \n",
    "                arg = params[:-2]\n",
    "                loc = params[-2]\n",
    "                scale = params[-1]\n",
    "\n",
    "                pdf = distribution.pdf(x, loc=loc, scale=scale, *arg)\n",
    "                sse = np.sum(np.power(y - pdf, 2.0))\n",
    "\n",
    "                try:\n",
    "                    if ax:\n",
    "                        pd.Series(pdf, x).plot(ax=ax)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "                if best_sse > sse > 0:\n",
    "                    best_distribution = distribution\n",
    "                    best_params = params\n",
    "                    best_sse = sse\n",
    "\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return best_distribution.name, best_params\n",
    "\n",
    "def make_pdf(dist, params, size=1000):\n",
    "    \"\"\"Generate distributions's Probability Distribution Function \"\"\"\n",
    "\n",
    "    arg = params[:-2]\n",
    "    loc = params[-2]\n",
    "    scale = params[-1]\n",
    "\n",
    "    start = dist.ppf(0.01, *arg, loc=loc, scale=scale) if arg else dist.ppf(0.01, loc=loc, scale=scale)\n",
    "    end = dist.ppf(0.99, *arg, loc=loc, scale=scale) if arg else dist.ppf(0.99, loc=loc, scale=scale)\n",
    "\n",
    "    x = np.linspace(start, end, size)\n",
    "    y = dist.pdf(x, loc=loc, scale=scale, *arg)\n",
    "    pdf = pd.Series(y, x)\n",
    "\n",
    "    return pdf\n",
    "\n",
    "data = df.Vp\n",
    "\n",
    "# Make the \"All dists\" plot.\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = data.plot(kind='hist', bins=20, density=True, alpha=0.5)\n",
    "dataYLim = ax.get_ylim()\n",
    "\n",
    "best_fit_name, best_fit_params = best_fit_distribution(data, 20, ax, skip=['levy_stable'])\n",
    "best_dist = getattr(st, best_fit_name)\n",
    "\n",
    "ax.set_ylim(dataYLim)\n",
    "ax.set_title('All fitted distributions')\n",
    "ax.set_xlabel('Vp')\n",
    "ax.set_ylabel('Frequency')\n",
    "\n",
    "# Make plot with best params \n",
    "pdf = make_pdf(best_dist, best_fit_params)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = pdf.plot(lw=2, label='PDF', legend=True)\n",
    "data.plot(kind='hist', bins=20, normed=True, alpha=0.5, label='Data', legend=True, ax=ax)\n",
    "\n",
    "param_names = (best_dist.shapes + ', loc, scale').split(', ') if best_dist.shapes else ['loc', 'scale']\n",
    "param_str = ', '.join(['{}={:0.2f}'.format(k,v) for k,v in zip(param_names, best_fit_params)])\n",
    "dist_str = '{}({})'.format(best_fit_name, param_str)\n",
    "\n",
    "ax.set_title('Best fit distribution\\n' + dist_str)\n",
    "ax.set_xlabel('Vp')\n",
    "ax.set_ylabel('Frequency')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "<div>\n",
    "<img src=\"https://avatars1.githubusercontent.com/u/1692321?s=50\"><p style=\"text-align:center\">© Agile Geoscience 2020</p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "geoph",
   "language": "python",
   "name": "geoph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
