{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass classification\n",
    "**Before this notebook, you should look at [Pandas_for_ML_data_management.ipynb](Pandas_for_ML_data_management.ipynb).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The machine learning iterative loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"../images/ML_loop.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/ML_loop.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The lithology prediction task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/machine_learning_primer.png\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"../images/machine_learning_primer.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('https://geocomp.s3.amazonaws.com/data/Panoma_training_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Well Name'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make `X` and `y`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "### EXERCISE\n",
    "\n",
    "**Make the `X` and `y` arrays we'll use for training.** `X` is going to be the columns `'GR','RHOB','PE','ILD_log10'` and `y` will be the `'Lithology'` column. Cast the data to NumPy arrays. <a title=\"Use ordinary dict-style indexing to get the columns out of the DataFrame. Use the values attribute of the columns to get the equivalent NumPy array.\"><b>Hover for HINT</b></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "features = ['GR','RHOB','PE','ILD_log10']\n",
    "\n",
    "# You *can* leave these as Pandas objects, but I prefer NumPy arrays.\n",
    "X = df[features].values\n",
    "y = df['Lithology'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, M = X.shape\n",
    "assert N >= 3000\n",
    "assert M == 4\n",
    "assert y.size == N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must split the data into a training set, a validation set, and a test set. **This is a key step in the process.**\n",
    "\n",
    "Scikit-learn has a function called `train_test_split`:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_, y_train, y_ = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "```\n",
    "\n",
    "but we are't going to use it because it violates the IID assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we feel about this? We've drawn all our records randomly from our data.\n",
    "\n",
    "### The IID assumption\n",
    "\n",
    "Our data records are not strictly independent and identically distributed. So splitting like this is not a great idea for these data. We should split by well instead.\n",
    "\n",
    "[No-one is immune to this kind of error!](https://twitter.com/andrewyng/status/931026446717296640)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the `train_test_split` function splits your `X` and `y` into two groups. What we need is three groups: train, validation, and test. The training set is for *building* the model(s), the validation set is for scoring the model(s) and selecting hyperparameters (sometimes also called the **dev** set), and the *test* set is used one to assess the likely real-world performance of the trained model.\n",
    "\n",
    "Note that you should only predict on the **test** set once, at the end of model selection and tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the I.I.D assumption was valid then we could get our `train`, `val`, and `test` sets using the `train_test_split` function twice like so:\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_, y_train, y_ = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_, y_, test_size=0.5, random_state=42)\n",
    "```\n",
    "But since I.I.D does not hold we can pull our samples on a well by well basis. In doing so we must make sure that we have similar proportions of lithologies in each of the three sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "source": [
    "## MultiIndex\n",
    "\n",
    "Waht if you have a multi-index?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "dg = df.copy()\n",
    "\n",
    "dg = dg.set_index(['Well Name', 'Depth'])\n",
    "\n",
    "dg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(dg['Lithology'], dg.index.get_level_values('Well Name'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "### EXERCISE\n",
    "\n",
    "What is the distribution of lithologies across the wells? <a title=\"Check out pd.crosstab\"><b>Hover for HINT</b></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "pd.crosstab(df['Lithology'], df['Well Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "ct = pd.crosstab(df['Lithology'], df['Well Name'])\n",
    "\n",
    "# Let's order this crosstab so that the validation and test wells are on the far right.\n",
    "# Also, let's set the order of the lithologies in the table.\n",
    "train_wells = ['NEWBY', 'CROSS H CATTLE', 'LUKE G U', 'NOLAN', 'SHRIMPLIN']\n",
    "val_wells = ['CHURCHMAN BIBLE', 'SHANKLE']\n",
    "\n",
    "lith_order = ['sandstone', 'siltstone', 'mudstone', 'wackestone', 'limestone', 'dolomite']\n",
    "\n",
    "# Reindex the columns and the row.\n",
    "ct = ct[train_wells + val_wells].reindex(lith_order)\n",
    "\n",
    "# Normalize to percentages by row.\n",
    "ct_norm = 100 * ct.div(ct.sum(axis=1), axis=0)\n",
    "\n",
    "# Apply the bar style.\n",
    "# ct_norm.style.bar(color=['#d65f5f', '#5fba7d'], vmax=0.5)\n",
    "\n",
    "# Get really fancy: apply style from normalized ct to the un-normalized one.\n",
    "def make_bar_style(x):\n",
    "    c = f\"#5fba7d {x}%\" if x > 10 else f\"#ca3f3f {x}%\"  # Green if > 10% else red.\n",
    "    return f\"background: linear-gradient(270deg,{c}, transparent {x}%); width: 10em\"    \n",
    "\n",
    "ct.style.apply(lambda x: ct_norm.applymap(make_bar_style), axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "color_codes = {'dolomite': '#C186C1',\n",
    "              'limestone': '#2E86C1',\n",
    "              'wackestone': '#728393',\n",
    "              'mudstone': '#C1C1C1',\n",
    "              'siltstone': '#F3E8BF',\n",
    "              'sandstone': '#F4D03F'}\n",
    "\n",
    "cmap_facies = ListedColormap(color_codes.values(), 'indexed')\n",
    "ax = ct.T.plot(kind='bar', stacked=True, rot=0, figsize=(10,6), cmap=cmap_facies)\n",
    "_ = ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's split!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Well Name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_wells = ['NEWBY', 'CROSS H CATTLE', 'LUKE G U', 'NOLAN', 'SHRIMPLIN']\n",
    "\n",
    "X_train = df.loc[df['Well Name'].isin(train_wells), features].values\n",
    "y_train = df.loc[df['Well Name'].isin(train_wells), 'Lithology'].values\n",
    "\n",
    "# And we might need this later...\n",
    "wells_train = df.loc[df['Well Name'].isin(train_wells), 'Well Name'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_wells = ['CHURCHMAN BIBLE', 'SHANKLE']\n",
    "\n",
    "X_ = df.loc[df['Well Name'].isin(val_wells), features].values\n",
    "y_ = df.loc[df['Well Name'].isin(val_wells), 'Lithology'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_.shape, y_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_, y_, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option is k-fold cross-validation, or LOGO (leave one group out) cross-validation. But if you have enough training data, it's better to stick to holdout sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check the distributions\n",
    "\n",
    "Let's visualize the distributions over the features, for each of the datasets. We'd like the distributions to match, i.e. training matches val and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(ncols=4, figsize=(14,3))\n",
    "\n",
    "for ax, feature, *datasets in zip(axs, features, X_train.T, X_val.T, X_test.T):\n",
    "    for dataset in datasets:\n",
    "        sns.kdeplot(dataset, ax=ax)\n",
    "    ax.set_ylabel(None)\n",
    "    ax.set_title(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardizing the data\n",
    "\n",
    "Some methods don't care about the scale of the features. For example, decision trees and random forests treat each feature independently.\n",
    "\n",
    "However, most methods &mdash; for example those relying on distance (e.g. KNN), or on linear combinations (e.g. linear regression), or on squeezing functions (e.g. neural nets) &mdash; require the data to be normalized or standardized:\n",
    "\n",
    "- **Normalization**: scaling to a range of [0, 1] or [-1, +1] for example. As a rule of thumb, you might do this for uniformly distributed data.\n",
    "- **Standardization**: scaling to a zero mean and unit variance (also known as Z-score). Do this for normally (Gaussian) distributed data.\n",
    "\n",
    "It's likely a good idea to scale it no matter which method you try.\n",
    "\n",
    "`scikit-learn` has lots of scalers. The `StandardScaler` removes the mean and scales the data to unit variance.\n",
    "\n",
    "Let's take a quick look at the data before scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X_train[:,0], X_train[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that interpreting this as a Euclidean space — which is how a lot of machine learning models are going to look at it — seems to distort the data, purely because of the (potentially arbitrary, as here) difference in scales across the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0, x1 = X[20, :2], X[2490, :2]\n",
    "\n",
    "plt.scatter(X_train[:,0], X_train[:,1])\n",
    "plt.scatter(*x0.T, c='r')\n",
    "plt.scatter(*x1.T, c='r')\n",
    "# plt.axis('equal')  # <-- Now uncomment this line to see how Euclid sees it!\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit a scaler to the training data, then transform all the datasets, with a pattern that will become very familiar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it is possible to fit and transform some models in one step:\n",
    "\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "You could even instantiate the model instance in the same statement, but this has the major disadvantage of not getting access to the model later.\n",
    "\n",
    "Now let's plot the data using equal axes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = scaler.transform(X)[20, :2]\n",
    "s1 = scaler.transform(X)[2490, :2]\n",
    "\n",
    "plt.scatter(X_train[:,0], X_train[:,1])\n",
    "plt.scatter(*s0.T, c='r')\n",
    "plt.scatter(*s1.T, c='r')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is better.\n",
    "\n",
    "Let's scale the validation data too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "# We won't transform X_test for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also reconfirm that our training and validation sets have roughly the same distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train[:,0], X_train[:,1])\n",
    "plt.scatter(X_val[:,0], X_val[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since any future application of the model will need this scaler, you may need to share it with others who want to apply the model. There are two options for this:\n",
    "\n",
    "- Share the scaler as a separate 'model'.\n",
    "- Make an `sklearn` pipeline containing both the scaler and the classifier.\n",
    "\n",
    "We'll do both of these eventually. For now, we'll just keep this scaler in memory and move on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**\n",
    "\n",
    "Data normalization doesn't magically fix problems with the data. If you're lumping a bunch of well data together and the GR, say, is calibrated differently in each well, or the RHOB units vary, then the scaled data will also have this problem. So you still need to QC your data to ensure it's internally consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Should you scale the target?\n",
    "\n",
    "There is usually no advantage to scaling the target, and it does make model evaluation a bit more fiddly, so we generally don't do it.\n",
    "\n",
    "The exception is for some neural network approaches. Very large magnitudes may result in 'exploding gradients', which in turn may result in 'saturation' (for some activation functions), or in very large updates to the parameters. Either way, the network will struggle to learn. So in these cases, we will sometimes scale a regression target for a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## A simple model: _k_-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "print(sklearn.__version__)\n",
    "\n",
    "# Make sure we can see all of the model details.\n",
    "sklearn.set_config(print_changed_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fairly naive, **non-parametric** method for classifying data is the _k-nearest neighbours_ algorithm. 'Non-parametric' means that, strictly speaking, it's not quite machine learning: no parameters are being learned during training. The algorithm simply remembers all the data and uses it to do spatial queries for new data points.\n",
    "\n",
    "The label of the object in question is determined by the neighbouring data points in the feature space used. Its most important parameter, *k*, called `n_neighbors` in the `sklearn` library, is the number of neighbours you include to make a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "from mlutils import plot_knn\n",
    "\n",
    "plot_knn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to do some machine learning. First we import, then instantiate, the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `fit` (train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next block is all you need to train a classifier model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "y_pred == y_val\n",
    "\n",
    "# Then: (y_pred == y_val) / y_pred.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "for pair in zip(y_pred, y_val):\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 2))\n",
    "plt.plot(y_val[:100], 'o-')\n",
    "plt.plot(y_pred[:100], 'o-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach might involve plotting, and often we'd like integers anyway. \n",
    "\n",
    "We could use the `LabelEncoder` in sklearn, but I want to control the order, so let's do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def label_encode(array, classes):\n",
    "    return np.array(list(map(list(classes).index, array)))\n",
    "\n",
    "classes = {\n",
    "    'sandstone': '#F4D03F',\n",
    "    'siltstone': '#F3E8BF' ,\n",
    "    'mudstone': '#C1C1C1',\n",
    "    'wackestone': '#728393',\n",
    "    'limestone': '#2E86C1',\n",
    "    'dolomite': '#C186C1',\n",
    "}\n",
    "\n",
    "print(label_encode(classes, classes.keys()))\n",
    "\n",
    "y_val_int = label_encode(y_val, classes.keys())\n",
    "y_pred_int = label_encode(y_pred, classes.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "plt.imshow(y_val_int.reshape(-1, 1), aspect=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(5, 10), sharey=True)\n",
    "\n",
    "cmap_facies = ListedColormap(classes.values(), 'indexed')\n",
    "\n",
    "ax = axs[0]\n",
    "im = ax.imshow(y_val_int[:100].reshape(-1, 1), aspect='auto', cmap=cmap_facies, vmin=-0.5, vmax=5.5)\n",
    "ax.set_title('Actual')\n",
    "\n",
    "ax = axs[1]\n",
    "im = ax.imshow(y_pred_int[:100].reshape(-1, 1), aspect='auto', cmap=cmap_facies, vmin=-0.5, vmax=5.5)\n",
    "ax.set_title('Predicted')\n",
    "\n",
    "ax = axs[2]\n",
    "ax.axis('off')\n",
    "cbar = fig.colorbar(im, ticks=[0, 1, 2, 3, 4, 5])\n",
    "cbar.ax.set_yticklabels(classes.keys())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may not be any no dolomite in this *part* of the prediction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...or any of it! (Your mileage may vary: our datasets and random splits may differ here.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How did we do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the results is great, but we need to get quantitative if we want to make sure that the model we trained is _good_ and produces reasonable results. \n",
    "\n",
    "Let's make sure we do well on the **training** data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = clf.score(X_train, y_train)\n",
    "print(f\"The training accuracy is {score*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic *fair* test is to look at how many good predictions we would make if we predict on our **validation** data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = clf.score(X_val, y_val)\n",
    "print(f\"The validation accuracy is {score*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same as explicitly calling `sklearn.metrics.accuracy_score()` on the validation labels and the prediction from the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy is (usually) not enough\n",
    "\n",
    "The _accuracy_ is just one of the _metrics_ we can use to check the quality of the predictions. There are a large number of different metrics and depending on your data and problem you may need to find the one that adjusts better to your needs.\n",
    "\n",
    "In a binary classification with balanced classes, the accuracy score is useful. But... in general accuracy can be misleading, especially in datasets with unbalanced classes. A more robust metric is the `F1` metric. It combines the `precision` score and `recall` for each class:\n",
    "\n",
    "$$ \\mathrm{F1} = \\frac{2}{\\frac{1}{\\mathrm{precision}}+ \\frac{1}{\\mathrm{recall}}} $$\n",
    "\n",
    "Scikit-learn gives a nice summary of these three metrics using `classification_report`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_val, clf.predict(X_val), digits=3, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What on earth is macro average?\n",
    "\n",
    "From [this nice answer](https://datascience.stackexchange.com/a/24051/70391) on SO:\n",
    "\n",
    ">  A macro-average will compute the metric independently for each class and then take the average (hence treating all classes equally), whereas a micro-average [`sklearn` calls it the weighted average] will aggregate the contributions of all classes to compute the average metric. In a multi-class classification setup, micro-average [weighted average] is preferable if you suspect there might be class imbalance.\n",
    "\n",
    "Since we almost always have imbalanced classes, we should usually prefer the weighted average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with class imbalance\n",
    "\n",
    "\"Class imbalance\" means that we have more of some labels than others. This does lead to problems in both training and evaluation.\n",
    "\n",
    "- Collect more data, if it's an option.\n",
    "- Choose correct evaluation metrics (some attempt to handle imbalance, eg F1 vs accuracy).\n",
    "- Choose models that handle imbalance well, e.g. tree-based models. (Some implementations, like XGBoost, try to implicitly handle imbalance.)\n",
    "- When splitting into holdout or cross-validation datasets, use _stratified_ variants. Note that this is the default behaviour in Scikit-Learn's cross-validation functions, but _not_ in `train_test_split()`.\n",
    "- If the problem is an over-abundant class, split that class into groups, and mix with the rare class (which you repeat in each group).\n",
    "- If the problem is an over-abundant class, under-sample the abundant class(es) -- but this means losing the learning effect from those samples.\n",
    "- If the problem is an over-abundant class, cluster the abundant class into _r_ clusters (where _r_ is the number of instances in the rare class). In other words, you're sort of undersampling without losing the information.\n",
    "- If the problem is an _under_-abundant class, over-sample that rare class, e.g. by repeating or fuzzing (adding some random noise in each dimension).\n",
    "- Simulate synthetic samples for the rare class (e.g. with SMOTE).\n",
    "- Use a custom cost function, or a cost matrix, that penalizes errors on the rare class(es).\n",
    "\n",
    "I found this library, which could be useful if you're considering over- or under-sampling as a strategy: https://github.com/scikit-learn-contrib/imbalanced-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix\n",
    "\n",
    "The [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) is another option: which classes are being confused with which?  Scikit-learn has a function for that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But as you can see, it's not very clear... What does each row/column represent? There's another function that's a bit more informative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "plot_confusion_matrix(clf, X_val, y_val, labels=list(classes.keys()), cmap=plt.cm.Greens, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I like to normalize the values across the rows with `normalize='true'`, but then you only get proportions for the labels. To maintain the support numbers as above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(classes.keys())\n",
    "cm = confusion_matrix(y_val, y_pred, labels=labels)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "cmp = plot_confusion_matrix(clf, X_val, y_val, labels=labels, normalize='true', cmap=plt.cm.Greens, ax=ax)\n",
    "for n, p in zip(cm.ravel(), cmp.text_.ravel()):\n",
    "    p.set_text(f'{n}\\n[{p.get_text()}]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision-recall, sensitivity-specificity, etc.\n",
    "\n",
    "**Accuracy** is simply the number of correct predictions divided by the total number of predictions. The number and types of error don't come into it. There's one number for the entire prediction. And it omits a big part of the story.\n",
    "\n",
    "There are various ways to capture the kinds of errors the classifier makes. Confusingly, they all have synonyms too (shown here)...\n",
    "\n",
    "- **Precision, or positive predictive value**: of the instances predicted as class Ⓐ, what proportion were correct?\n",
    "- **Recall, sensitivity, hit rate, or true positive rate**: what proportion of all Ⓐ instances were correctly predicted as Ⓐ?\n",
    "- **Specificity, selectivity, or true negative rate ( = 1 - false positive rate)**: what proportion of all non-Ⓐ instances were correctly predicted as non-Ⓐ?\n",
    "\n",
    "We tend to consider either precision and recall, or sensitivity (the same as recall) and specificity. There's a trade-off between the two: better recall involves giving up some precision and vice-versa.\n",
    "\n",
    "The ROC curve tries to capture this trade-off. Look for [the binary classification notebook](Binary_classification.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtlety in the score\n",
    "\n",
    "Let's look at two types of error. \n",
    "\n",
    "Both of these are errors in the eyes of the ML algorithm, but not necessarily in the eyes of the geologist.\n",
    "\n",
    "First: predicting 8 (maybe coarse sand) instead of 7 (fine sand) should not be as bad as predicting 2 (say, basalt) instead of 7. How could we fix this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(\"../images/classification_errors.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The baseline: a dummy classifier\n",
    "\n",
    "The dummy classifier tries to answer the question, \"How well can I do with zero intelligence?\". In other words, given the relative proportions of facies, what would you expect from random weighted guesses?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "clf = DummyClassifier()\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_val)\n",
    "\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In essence, if I can't beat this, then my model is definitely not useful.\n",
    "\n",
    "Another measure of 'useful' is whether the classifier can beat a human (but don't forget that these labels were generated by a human!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on you requirements, this results might be good enough to deploy this model and use it in production but it is usually not the best model you can get. Each method has a set of controls or arguments, known as _hyperparameters_, that can be tweaked to tune the training.\n",
    "\n",
    "Note that we don't call these settings 'parameters' because that word is reserved for the learnable parameters, weights, or coefficients in the model (e.g. $\\mathbf{w}$ and $b$ in $\\hat{y} = \\mathbf{w}\\mathbf{x} + b$).\n",
    "\n",
    "For the `KNeighborsClassifier` there are a few of these hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this particular method, the most important hyperparameter to adjust is `n_neighbors` (it's the `k` in the `KNeighborsClassifier`). Unfortunately, there's no rule that tells you what's the optimal value of `k`. To overcome this we can train many models with different values of `k` and compare the results of classifications applied to the _Validation_ data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "k = np.arange(1, 60, 2)  # No need to test all values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop over each value in `k` and store the F1 score for each attempt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "vals, trns = [], []\n",
    "\n",
    "for ki in k:\n",
    "    clf = KNeighborsClassifier(n_neighbors=ki)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_ptrn = clf.predict(X_train)\n",
    "    trns.append(f1_score(y_train, y_ptrn, average='weighted'))\n",
    "\n",
    "    y_pred = clf.predict(X_val)\n",
    "    vals.append(f1_score(y_val, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What value of `k` gives us the best result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(k, trns, lw=3.0, label=\"Training F1 score\")\n",
    "plt.plot(k, vals, lw=3.0, label=\"Validation F1 score\")\n",
    "_ = plt.xlabel('n_neighbors')\n",
    "_ = plt.ylabel('clf.score')\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "### Exercise\n",
    "\n",
    "- Figure out `k_best`, the value of `k` corresponding to the best score in `vals`. <b><span title=\"You will find np.argmax() useful. It gives the position of the maximum value in an array. Try it on array([2,4,6,10,6,4,2]).\">Hover for hint.</span></b>\n",
    "- Create a new `KNeighborsClassifier` classifier where you specify the optimal number of neighbours.\n",
    "- Write a new classification report for the new classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "k_best = k[np.argmax(vals)]\n",
    "\n",
    "print(f\"k_best = {k_best}\\n\")\n",
    "\n",
    "clf = KNeighborsClassifier(k_best)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "\n",
    "print(classification_report(y_val, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Support vector machine\n",
    "\n",
    "The support vector machine is one of the classic linear classifiers. A linear SVM is a **parametric** model, meaning that it learns a relatively small number of model parameters during training. [See this link for more on this jargon.](https://sebastianraschka.com/faq/docs/parametric_vs_nonparametric.html)\n",
    "\n",
    "SVMs &mdash; linear and kernel-based &mdash; do well in high-dimensional data, but not sparse data. They also need all the features to be at the same scale, so they work well for things like pixel intensities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "### Exercise\n",
    "\n",
    "Implement a support vector classifier and checks its effectiveness on the validation data, e.g. by printing the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC(probability=True)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_val)\n",
    "\n",
    "print(classification_report(y_val, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many, but not all, models provide probabilistic classifications. When you call `predict()`, you're getting the `argmax` of this array†.\n",
    "\n",
    "---\n",
    "\n",
    "† Sort of... the probabilities do not necessarily exactly match the predictions made by the SVM, which depend on the distance to the decision hypersurface in a high-dimenional space. The process that estimates probability, Platt scaling, is not totally predictable; [see the documentation.](https://scikit-learn.org/stable/modules/svm.html#scores-and-probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "clf.predict_proba(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that by default, the SVM will use regularization (unlike the usual alpha, the C hyperparameter is **inversely** proportional to the amount of regularization, so use low C for smoother models), and kernel transformations (fitting non-linearly separable data).\n",
    "\n",
    "Let's see how the training and validation scores vary with C:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = [0.01, 0.03, 0.1, 0.3, 1.0, 3.0, 10, 30]\n",
    "trains, vals = [], []\n",
    "\n",
    "for C in Cs:\n",
    "    clf = SVC(C=C).fit(X_train, y_train)\n",
    "    trains.append(clf.score(X_train, y_train))\n",
    "    vals.append(clf.score(X_val, y_val))\n",
    "\n",
    "plt.plot(Cs, vals, 'o-', label=\"Val. score\")\n",
    "plt.plot(Cs, trains, 'o-', label=\"Train. score\")\n",
    "plt.xlabel('C (proportional to complexity)')\n",
    "plt.xscale('log')\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the range 0.3-ish is where we want to look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(C=0.5)  # Pick a good compromise.\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(classification_report(y_val, clf.predict(X_val), digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Decision tree\n",
    "\n",
    "Decision trees are another non-parametric model type (like KNN).\n",
    "\n",
    "They have the advantage of being more explainable than a lot of other model types. They tend to do quite well with little tuning, however they are prone to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "### Exercise\n",
    "\n",
    "Implement a decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=4)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_val)\n",
    "\n",
    "print(classification_report(y_val, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being able to see feature importances is a nice feature of decision trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and so is being able to see the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "_ = plot_tree(clf,\n",
    "              filled=True,\n",
    "              feature_names=features,\n",
    "              class_names=clf.classes_,\n",
    "              fontsize=8,\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees are prone to overtraining. An ensemble of decision trees can help avoid this... and makes for a highly effective classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "### Exercise\n",
    "\n",
    "Implement a Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=4).fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_val)\n",
    "\n",
    "print(classification_report(y_val, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Choosing the right estimator\n",
    "\n",
    "Often the hardest part of solving a machine learning problem can be finding the right estimator for the job.\n",
    "\n",
    "This is a good place to start ([here](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html) is a clickable version):\n",
    "\n",
    "<img src=\"https://scikit-learn.org/stable/_static/ml_map.png\"></img>\n",
    "\n",
    "---\n",
    "\n",
    "Different estimators are better suited for different types of data and different problems. For a classifier comparison (below) check the source code [here](http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html)\n",
    "\n",
    "<img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_classifier_comparison_001.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Check out this paper with a comparison of many classifiers](https://arxiv.org/abs/1708.05070)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "exe"
    ]
   },
   "source": [
    "### Exercise\n",
    "\n",
    "- Can you beat our current high score by twiddling hyperparameters on one of these models?\n",
    "- Or, try other methods available in the scikit-learn library. See the list [here](http://scikit-learn.org/stable/supervised_learning.html). For example, get a sneak peek at neural nets with the MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=[24, 12],\n",
    "                    learning_rate='constant',\n",
    "                    alpha=0.001,\n",
    "                    max_iter=5000,\n",
    "                    solver='adam',\n",
    "                    random_state=42,\n",
    "                   )\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_val)\n",
    "\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, ExtraTreesClassifier\n",
    "\n",
    "clf = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_val)\n",
    "\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the models can be improved (or worsened) by changing the parameters that internally make the method work. It's always a good idea to check the documentation of each model (e.g. `RandomForestClassifier` [docs](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)). This process is usually called _hyperparameter tuning_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn offers a simple way to test different parameters for each model through a function called `GridSearchCV`.\n",
    "\n",
    "The default behviour is to split the training data randomly into _k_ 'folds' (e.g. with `cv=6` in line 9 below). But our records are not IID (specifically, not independent) so we can't do this. We'll split across the wells instead by defining a 'LOGO' splitter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, LeaveOneGroupOut\n",
    "\n",
    "parameters = {'min_samples_leaf': np.arange(3, 19),\n",
    "              'max_depth': np.arange(2, 13)}\n",
    "\n",
    "logo = LeaveOneGroupOut().split(X_train, y_train, groups=wells_train)\n",
    "\n",
    "rfc = RandomForestClassifier(random_state=42)\n",
    "clf = GridSearchCV(rfc, parameters, cv=logo, n_jobs=6, verbose=3)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we do on each well as we omit it from the training in turn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, well in enumerate(np.unique(wells_train)):\n",
    "    scores = clf.cv_results_[f'split{i}_test_score']\n",
    "    print(f\"{well:>16s} ... {np.mean(scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the parameter space look with respect to the score of the classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = clf.cv_results_['mean_test_score']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 7.5))\n",
    "im = ax.imshow(scores.reshape((11, 16)),\n",
    "               origin='lower',\n",
    "               extent=[2.5, 18.5, 1.5, 12.5],\n",
    "               interpolation='none',\n",
    "               aspect=1,\n",
    "               cmap='viridis'\n",
    "              )\n",
    "ax.set_ylabel('max_depth')\n",
    "ax.set_xlabel('min_samples_leaf')\n",
    "cb = plt.colorbar(im, shrink=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`clf` can now tell us the best parameters to use with our `RandomForestClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nice thing about `scikit-learn`'s methods is that they're all consistent and behave in the same way. Notice how`GridSearchCV` was `.fit()`. That means that we can use it to `.predict()` and it will automatically use the best set of parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_val)\n",
    "print(classification_report(y_val, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_train, y_train), clf.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model application, and pipelines\n",
    "\n",
    "Don't forget that you'll also need the scaler that goes with the model. The best idea is probably to make a pipeline which contains both the scaler and the classifier; then you only have one model to share.\n",
    "\n",
    "Making pipelines is easy. We just have to provide (from the docs) **a list of (name, transform) tuples (implementing fit/transform) that are chained, in the order in which they are chained, with the last object an estimator.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "model = Pipeline(steps=[('scaler', scaler),\n",
    "                        ('classifier', clf),\n",
    "                       ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is 'just' an `sklearn` model.\n",
    "\n",
    "Now might be a good time to check our **test** set — the one we reserved at the start. How might our model do on this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)  # X_test was unscaled.\n",
    "\n",
    "f1_score(y_test, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a similar result to the validation set. That's good.\n",
    "\n",
    "The pipeline is convenient whenever we have multiple steps in the prediction workflow, because it ensures we'll be consistent and makes it easier to save the model (below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the final model\n",
    "\n",
    "Now that we have selected the model we want to use, we should retrain it on all of the data.\n",
    "\n",
    "NB We could omit the scaling step for a random forest. But it doesn't do any harm..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X)\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42, **clf.best_params_).fit(X, y)\n",
    "\n",
    "model = Pipeline(steps=[('scaler', scaler),\n",
    "                        ('classifier', rf),\n",
    "                       ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model persistence\n",
    "\n",
    "Often, we'd like to save the trained model, to go and apply it in some other application, or to share with someone else. The easiest way to save most models is as a Python 'pickle' object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(model, 'facies_pipeline.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you load and use a saved model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load('facies_pipeline.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.named_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can load some data:\n",
    "\n",
    "    df = pd.read_csv('..some_new_data_you_have_collected.csv')\n",
    "  \n",
    "Then extract the relevant features and cast it as a 2D array:\n",
    "\n",
    "    X_new = df[['GR','RHOB','PE','ILD_log10']].values\n",
    "\n",
    "Now pass the matrix `X_new` into the classifier's predict method:\n",
    "\n",
    "    y_pred = model.predict(X_new)\n",
    "\n",
    "And you're done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "<p style=\"color:gray\">©2020 Agile Geoscience. Licensed CC-BY.</p>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
